{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPjXrIAc0xkC8cZ7AHnFdmb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/euneun316/NLP-Study/blob/main/Natural%20Language%20Processing/Text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02.텍스트 전처리(Text preprocessing)\n",
        "- 풀고자 하는 문제의 용도에 맞게 텍스트를 사전에 처리하는 작업\n",
        "- 참조 Url : https://github.com/ukairia777/tensorflow-nlp-tutorial"
      ],
      "metadata": {
        "id": "GRzw_LZl5Vwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01) 토큰화(Tokenization)"
      ],
      "metadata": {
        "id": "shoCHgaGAXip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 합니다."
      ],
      "metadata": {
        "id": "oVcqYKDC1Tv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 단어 토큰화(Word Tokenization)\n",
        "- NLTK는 영어 코퍼스를 토큰화하기 위한 도구들을 제공\n",
        "- word_tokenize"
      ],
      "metadata": {
        "id": "83z1SE_j-hhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "id": "se6WFb5o7uZ7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5td_xf7w9327",
        "outputId": "e4f65d38-65a1-4332-a859-f1692dd89705"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 토큰화1 :',word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gMGxHtf9waH",
        "outputId": "87e1e8be-a29b-4ed7-ff6e-8b8be9840f02"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화1 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- wordPunctTokenizer"
      ],
      "metadata": {
        "id": "MLflZzsK-Z3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 토큰화2 :',WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPQgsy7--Mx4",
        "outputId": "08e97a46-7aa8-461c-994d-09ab4a040e45"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화2 : ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 케라스"
      ],
      "metadata": {
        "id": "ezfLaC3S-U1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 토큰화3 :',text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3t6FJ_q-Sls",
        "outputId": "4ca1f85a-8e5d-47d3-d459-38be3bc9897e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화3 : [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 표준 토큰화 예제\n",
        "- Penn Treebank *Tokenization*"
      ],
      "metadata": {
        "id": "hsx9zCT4_M_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "print('트리뱅크 워드토크나이저 :',tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhDdhGJt_HF0",
        "outputId": "b4a2e7b8-0543-4a2a-87a7-62596ee9d9d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "트리뱅크 워드토크나이저 : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문장 토큰화(Sentence Tokenization)"
      ],
      "metadata": {
        "id": "tsIVuPXR1biE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NLTK에서는 영어 문장의 토큰화를 수행하는 : sent_tokenize"
      ],
      "metadata": {
        "id": "CViVyl5r_fW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print('문장 토큰화1 :',sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKlHywOv_kU5",
        "outputId": "067536cb-eaa4-4d97-d3e2-d3646bf4dd8d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 문장 중간에 마침표가 다수 등장"
      ],
      "metadata": {
        "id": "F_b9TQv5_pG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print('문장 토큰화2 :',sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVmTB6kC_rNe",
        "outputId": "f126a8a3-0e60-450d-8c58-74249e41a81b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 박상길님이 개발한 KSS(Korean Sentence Splitter)"
      ],
      "metadata": {
        "id": "C75QUDd3_4RX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efgoeOKj_uoz",
        "outputId": "89f7843b-59c0-4304-aadf-450065c07a8e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-3.4.tar.gz (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 69.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from kss) (2019.12.20)\n",
            "Building wheels for collected packages: kss, emoji\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.4-py3-none-any.whl size=42449209 sha256=ac10efb8b04603e6cfe334f46e7ecab8db596ec3a5cc4912f30327701e03df00\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/8e/5b/305f0a804fba3943f353f1b0e3cb1fad39e4f5ae4893ea9590\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=44d7a68561972424628f5e73537afeecc93c9a0099d886d1d791e220f4930396\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
            "Successfully built kss emoji\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.6.3 kss-3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "\n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "print('한국어 문장 토큰화 :',kss.split_sentences(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x0MHBf6_zur",
        "outputId": "6a6174bc-a1f7-4d68-bbe2-00f3bdd62575"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Korean Sentence Splitter]: Initializing Pynori...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습"
      ],
      "metadata": {
        "id": "K1UA4fYlANRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NLTK"
      ],
      "metadata": {
        "id": "mPORpfWqBHeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brW6VInjAvw-",
        "outputId": "7d5f0f08-73f3-4edd-e4c2-4dd1d06a65c1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "tokenized_sentence = word_tokenize(text)\n",
        "\n",
        "print('단어 토큰화 :',tokenized_sentence)\n",
        "print('품사 태깅 :',pos_tag(tokenized_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I7wXuZlAr0h",
        "outputId": "8e037186-1726-45e1-f1fd-f0a809ea8160"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Okt"
      ],
      "metadata": {
        "id": "-CuaBQSaA1vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E3J71JzA7IJ",
        "outputId": "8081d8d8-633f-4563-ea5e-b3d367637fa5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 591 kB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 89.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "okt = Okt()\n",
        "kkma = Kkma()\n",
        "\n",
        "print('OKT 형태소 분석 :',okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print('OKT 품사 태깅 :',okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print('OKT 명사 추출 :',okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8lZvoiCA6PN",
        "outputId": "b29d8423-727e-4b75-fbb2-bd9b2bfd27cc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
            "OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
            "OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 꼬꼬마"
      ],
      "metadata": {
        "id": "Hdjx-sPfBN0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('꼬꼬마 형태소 분석 :',kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print('꼬꼬마 품사 태깅 :',kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print('꼬꼬마 명사 추출 :',kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCfLw8LGBU_A",
        "outputId": "2fb58406-69be-4c09-f8c2-e02042222e9d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
            "꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
            "꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02) 정제(Cleaning) and 정규화(Normalization)"
      ],
      "metadata": {
        "id": "KfmF_UI4B3C9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 영어는 길이가 2~3 이하인 단어를 제거하는 것만으로도 크게 의미를 갖지 못하는 단어를 줄이는 효과를 갖고있습니다."
      ],
      "metadata": {
        "id": "xtRWOW16C5pI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "\n",
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "print(shortword.sub('', text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9UpvKV6B6Lt",
        "outputId": "eab18429-edcd-431c-c0bf-5cc2ba4551ae"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03) 어간 추출(Stemming) and 표제어 추출(Lemmatization)"
      ],
      "metadata": {
        "id": "TYaelMhVCM3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "눈으로 봤을 때는 서로 다른 단어들이지만, 하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이겠다는 것"
      ],
      "metadata": {
        "id": "V-Wr0end134L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 표제어 추출(Lemmatization)"
      ],
      "metadata": {
        "id": "UOCZ3SF1ELCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "표제어(Lemma)는 한글로는 '표제어' 또는 '기본 사전형 단어' 정도의 의미"
      ],
      "metadata": {
        "id": "GmnJuaVh17QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Pb2vlzEF_S",
        "outputId": "e6b54bed-b8be-4c0b-89da-bf7f1bc978b2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "\n",
        "print('표제어 추출 전 :',words)\n",
        "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0606hDbiEELz",
        "outputId": "77e6a931-2aa5-4349-ba59-b52c397b187b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- WordNetLemmatizer"
      ],
      "metadata": {
        "id": "jxWJdTR3Eb7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('dies', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h5ud8DewEXQ1",
        "outputId": "4a37b25b-9921-4b5c-d8f0-45e937c012ba"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'die'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('watched', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vCnHRnxrEejk",
        "outputId": "36bd9d34-d98b-4457-e3ba-01de7a9006f2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'watch'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('has', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wR3aPYOqEgQm",
        "outputId": "775f654e-4db7-4ed2-f3b9-27aff62defdf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 어간 추출(Stemming)"
      ],
      "metadata": {
        "id": "QSjuUWMvEiRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업"
      ],
      "metadata": {
        "id": "SVArjPb-2Bpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 포터 알고리즘(Porter Algorithm)"
      ],
      "metadata": {
        "id": "6K5FrSLNFgwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "tokenized_sentence = word_tokenize(sentence)\n",
        "\n",
        "print('어간 추출 전 :', tokenized_sentence)\n",
        "print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYZzMmxIFYFe",
        "outputId": "6ae0be4f-9c3b-4c83-8616-47f037d8b71b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
            "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['formalize', 'allowance', 'electricical']\n",
        "\n",
        "print('어간 추출 전 :',words)\n",
        "print('어간 추출 후 :',[stemmer.stem(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSZ63_A7FcKN",
        "outputId": "c6992b42-a8a5-4b7b-d1e1-f2969aa47224"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['formalize', 'allowance', 'electricical']\n",
            "어간 추출 후 : ['formal', 'allow', 'electric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer) 알고리즘을 지원"
      ],
      "metadata": {
        "id": "t_Xew6KnFoDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print('어간 추출 전 :', words)\n",
        "print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\n",
        "print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acLUZvNeGFWk",
        "outputId": "ee437282-3685-4e80-8bfc-9da573ced608"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
            "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 한국어에서의 어간 추출"
      ],
      "metadata": {
        "id": "kudEfi2jGOVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04) 불용어(Stopword)"
      ],
      "metadata": {
        "id": "orKsrV8bHiA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요"
      ],
      "metadata": {
        "id": "HzDUhMvl1yoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "fWs9-zONHjwX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. NLTK에서 불용어 확인하기"
      ],
      "metadata": {
        "id": "5USgMVK7HwD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P6UFPnSHzsU",
        "outputId": "37f5d5c6-3f76-4d20-d3ef-c85341500c42"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_list = stopwords.words('english')\n",
        "print('불용어 개수 :', len(stop_words_list))\n",
        "print('불용어 10개 출력 :',stop_words_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5ya0wd4HxLC",
        "outputId": "ad932e6a-600d-4045-c830-933a95f31c22"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 : 179\n",
            "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. NLTK를 통해서 불용어 제거하기"
      ],
      "metadata": {
        "id": "pgzxEwLdH5fX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for word in word_tokens: \n",
        "    if word not in stop_words: \n",
        "        result.append(word) \n",
        "\n",
        "print('불용어 제거 전 :',word_tokens) \n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-B5yMHEH6M5",
        "outputId": "972d22ae-0848-4bcf-e210-76b20e161db0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 한국어에서 불용어 제거하기"
      ],
      "metadata": {
        "id": "cGeBAQIQICsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "\n",
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
        "\n",
        "stop_words = set(stop_words.split(' '))\n",
        "word_tokens = okt.morphs(example)\n",
        "\n",
        "result = [word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print('불용어 제거 전 :',word_tokens) \n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5rr1MZWIDxz",
        "outputId": "6c04a4e3-233c-4c29-d1dc-e6dbb67cbd34"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05) 정규 표현식(Regular Expression)"
      ],
      "metadata": {
        "id": "vT3S2zN3IbuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "xB0nxL1lIn-0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) .기호"
      ],
      "metadata": {
        "id": "2HCXsbi_I2vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"a.c\")\n",
        "r.search(\"kkk\") # 아무런 결과도 출력되지 않는다."
      ],
      "metadata": {
        "id": "A-ghdvNrIpji"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"abc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncTIDyD0Ivu3",
        "outputId": "2c920048-aeb2-4a10-b94b-866c7f024aa0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) ?기호"
      ],
      "metadata": {
        "id": "1Sqhpy8VI6gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab?c\")\n",
        "r.search(\"abbc\") # 아무런 결과도 출력되지 않는다."
      ],
      "metadata": {
        "id": "0dv3PCoyIy9d"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"abc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqbgKPK7JB9q",
        "outputId": "632cd428-8909-4906-dcf4-a48df78a310d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"ac\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdlmfVqBJGLh",
        "outputId": "82976a9d-7a0a-46d5-921b-2e6fd10ca27e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ac'>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) *기호"
      ],
      "metadata": {
        "id": "SXQkwt83JJIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab*c\")\n",
        "r.search(\"a\") # 아무런 결과도 출력되지 않는다."
      ],
      "metadata": {
        "id": "esacixDvJQaW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"ac\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrcjSOQHJSWq",
        "outputId": "cffbab56-95a0-4a9c-dccf-df482687bc8f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ac'>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"abc\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEBz48h-JUTl",
        "outputId": "0e4e696d-3a52-49e3-f834-31b42af15d47"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"abbbbc\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zGjitM4JVtl",
        "outputId": "6eea8d86-798a-4505-b2f0-482b94121bdb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 6), match='abbbbc'>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) +기호"
      ],
      "metadata": {
        "id": "_0_gAfU9JXRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab+c\")\n",
        "r.search(\"ac\") # 아무런 결과도 출력되지 않는다."
      ],
      "metadata": {
        "id": "khrV4fVcJcDs"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"abc\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBySeBFNJdy8",
        "outputId": "f95ec15e-a1a7-4e3e-9bdf-81b288558e29"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"abbbbc\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxmyL876JfLG",
        "outputId": "ac2fe54f-f2c4-4afd-d558-3fc3bdd47054"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 6), match='abbbbc'>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) ^기호"
      ],
      "metadata": {
        "id": "k4fPlUb5Jg2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"^ab\")\n",
        "\n",
        "# 아무런 결과도 출력되지 않는다.\n",
        "r.search(\"bbc\")\n",
        "r.search(\"zab\")"
      ],
      "metadata": {
        "id": "uZgU7VtGJl24"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"abz\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6msQ4MXcJoBm",
        "outputId": "78ad3134-7f16-4ec2-a9a5-d8536904d1c0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ab'>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) {숫자} 기호"
      ],
      "metadata": {
        "id": "iHb9gMUvJp4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab{2}c\")\n",
        "\n",
        "# 아무런 결과도 출력되지 않는다.\n",
        "r.search(\"ac\")\n",
        "r.search(\"abc\")\n",
        "r.search(\"abbbbbc\")"
      ],
      "metadata": {
        "id": "oT0F-GAJJvm6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"abbc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFms3kdeJyPG",
        "outputId": "7edc611b-29a9-4195-f639-3238cdc71a63"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 4), match='abbc'>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) {숫자1, 숫자2} 기호"
      ],
      "metadata": {
        "id": "sjk7fWXoJ0Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab{2,8}c\")\n",
        "\n",
        "# 아무런 결과도 출력되지 않는다.\n",
        "r.search(\"ac\")\n",
        "r.search(\"abc\")\n",
        "r.search(\"abbbbbbbbbc\")"
      ],
      "metadata": {
        "id": "FuXXJBKPJ673"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"abbc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rpOgZhQJ-lr",
        "outputId": "1ad0df6b-9eff-47d7-9e2e-ba29a99efe71"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 4), match='abbc'>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"abbbbbbbbc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc2kmUW-J_2j",
        "outputId": "0c9940ff-3fc6-4140-9a10-acb25c7820de"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 10), match='abbbbbbbbc'>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) {숫자,} 기호"
      ],
      "metadata": {
        "id": "R0ROFecPKBeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"a{2,}bc\")\n",
        "\n",
        "# 아무런 결과도 출력되지 않는다.\n",
        "r.search(\"bc\")\n",
        "r.search(\"aa\")"
      ],
      "metadata": {
        "id": "xbG3qe2ZKImk"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"aabc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF34kWGoKN6C",
        "outputId": "161a3b15-f174-404d-f60c-979b08b84d28"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 4), match='aabc'>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"aaaaaaaabc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vGvr8oWKQZa",
        "outputId": "bb2719d7-0c13-4063-b6fe-231c668b8c3d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 10), match='aaaaaaaabc'>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) [ ] 기호"
      ],
      "metadata": {
        "id": "dDIKZRnrKR1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"[abc]\") # [abc]는 [a-c]와 같다.\n",
        "r.search(\"zzz\") # 아무런 결과도 출력되지 않는다."
      ],
      "metadata": {
        "id": "Z0AE-dcfKX99"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe9rcYDJKZcL",
        "outputId": "40d24c95-7fcd-4c42-db23-8479ffe388b5"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='a'>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"aaaaaaa\")          "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zlm7D6RQKb4X",
        "outputId": "a0f23510-6e7c-4a6c-f520-58d3bef14beb"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='a'>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"baac\")     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcWkGbCLKdE-",
        "outputId": "da2f1f75-c9eb-4ff7-b0e3-5cdb2b0036d5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='b'>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"[a-z]\")\n",
        "\n",
        "# 아무런 결과도 출력되지 않는다.\n",
        "r.search(\"AAA\")\n",
        "r.search(\"111\") "
      ],
      "metadata": {
        "id": "fL4FSugrKep8"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"aBC\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1cw8gKAKf9T",
        "outputId": "304c843c-e9ab-4c65-dcaa-86ec180fd788"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='a'>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) [^문자] 기호"
      ],
      "metadata": {
        "id": "dB_eIS5zKhe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"[^abc]\")\n",
        "\n",
        "# 아무런 결과도 출력되지 않는다.\n",
        "r.search(\"a\")\n",
        "r.search(\"ab\") \n",
        "r.search(\"b\")"
      ],
      "metadata": {
        "id": "neFZV7YUKmrU"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"d\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFy326iaKoMy",
        "outputId": "594d337b-6d7e-4d67-acbf-27701b818528"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='d'>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"1\")   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgDxzBucKpv2",
        "outputId": "141cc3a6-fcce-49db-c508-a13a054fb4b5"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='1'>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 정규 표현식 모듈 함수 예제"
      ],
      "metadata": {
        "id": "IJ8YXS97KriF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) re.match() 와 re.search()의 차이"
      ],
      "metadata": {
        "id": "j0Opb3dQK936"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab.\")\n",
        "r.match(\"kkkabc\") # 아무런 결과도 출력되지 않는다."
      ],
      "metadata": {
        "id": "ueZYrxmlKsue"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search(\"kkkabc\")  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdoNI5N0K5MB",
        "outputId": "23556b02-2e51-4d80-9b60-df7545a6b889"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(3, 6), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.match(\"abckkk\")  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3et8rS_QK77V",
        "outputId": "d4b3a52d-be00-493b-9fd4-c88b2d66fdba"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) re.split()"
      ],
      "metadata": {
        "id": "mnDfrlGtLBqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 공백 기준 분리\n",
        "text = \"사과 딸기 수박 메론 바나나\"\n",
        "re.split(\" \", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgeITzXOLFe2",
        "outputId": "f7b93f06-9a9f-475b-821c-f8f94d150454"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사과', '딸기', '수박', '메론', '바나나']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 줄바꿈 기준 분리\n",
        "text = \"\"\"사과\n",
        "딸기\n",
        "수박\n",
        "메론\n",
        "바나나\"\"\"\n",
        "\n",
        "re.split(\"\\n\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLXTHLpsLIPd",
        "outputId": "94a2a8a1-f2c3-4407-f2c6-982aa44a9df0"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사과', '딸기', '수박', '메론', '바나나']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# '+'를 기준으로 분리\n",
        "text = \"사과+딸기+수박+메론+바나나\"\n",
        "\n",
        "re.split(\"\\+\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLi4NYXSLLtg",
        "outputId": "fcf73161-4cb4-4b8e-fba1-62806abf31a3"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사과', '딸기', '수박', '메론', '바나나']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) re.findall()"
      ],
      "metadata": {
        "id": "tLoWbvwYLNwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"이름 : 김철수\n",
        "전화번호 : 010 - 1234 - 1234\n",
        "나이 : 30\n",
        "성별 : 남\"\"\"\n",
        "\n",
        "re.findall(\"\\d+\", text) #숫자"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHdFi8CWLSZE",
        "outputId": "32712454-223b-430e-e581-870dbc6d4977"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['010', '1234', '1234', '30']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"\\d+\", \"문자열입니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLBbFz98LZ_j",
        "outputId": "04fbe637-deac-4609-a000-82bb42b63657"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4) re.sub()"
      ],
      "metadata": {
        "id": "Yohp5G0aLekC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
        "\n",
        "preprocessed_text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb7pQ_lcLhwu",
        "outputId": "5d411528-43c3-4ad3-cdb1-7ad68ec288b7"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 정규 표현식 텍스트 전처리 예제"
      ],
      "metadata": {
        "id": "UGiaooi2LuHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"100 John    PROF\n",
        "101 James   STUD\n",
        "102 Mac   STUD\"\"\""
      ],
      "metadata": {
        "id": "ytusGjjgLwPn"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.split('\\s+', text)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1ndjq_WLy-z",
        "outputId": "c08c7af6-58d7-4859-cdb4-6a80c085a3e0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall('\\d+',text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlcBVabqL2q2",
        "outputId": "fe6e0c0a-9d2f-4bf0-8f77-317d8c52cd8e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100', '101', '102']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall('[A-Z]',text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj9IJV3vL4la",
        "outputId": "764d9b58-90b8-4e29-aa72-41dc0b4ba947"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall('[A-Z]{4}',text)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oALsQhBoL6MB",
        "outputId": "70032550-24d7-44f6-8b48-1e5720589733"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PROF', 'STUD', 'STUD']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall('[A-Z][a-z]+',text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU12BhjXL-BR",
        "outputId": "ac73a8c9-2ee2-4b17-aa30-656119cc558d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['John', 'James', 'Mac']"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 정규 표현식을 이용한 토큰화"
      ],
      "metadata": {
        "id": "-5TOJBteMAKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"\n",
        "\n",
        "tokenizer1 = RegexpTokenizer(\"[\\w]+\")\n",
        "tokenizer2 = RegexpTokenizer(\"\\s+\", gaps=True)\n",
        "\n",
        "print(tokenizer1.tokenize(text))\n",
        "print(tokenizer2.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G4rDPMiMNP_",
        "outputId": "97ab8ac2-8444-424c-c66e-937d1b47db3a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
            "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06) 정수 인코딩(Integer Encoding)"
      ],
      "metadata": {
        "id": "HAdOYw1xo1pb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트를 숫자로 바꾸는 여러가지 기법들이 있습니다.<br>\n",
        "각 단어를 고유한 정수에 맵핑(mapping)시키는 전처리 작업이 필요할 때가 있습니다."
      ],
      "metadata": {
        "id": "djwQ3JW7wSpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 정수 인코딩(Integer Encoding)"
      ],
      "metadata": {
        "id": "P7c3mO6r3E8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어에 정수를 부여하는 방법 중 하나로 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법이 있습니다."
      ],
      "metadata": {
        "id": "WOxYGYbX3IQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 dictionary 사용하기"
      ],
      "metadata": {
        "id": "APW0NZL75efQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "jvidcnqS5leg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ],
      "metadata": {
        "id": "kBLnSXg45nbV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "기존의 텍스트 데이터가 문장 단위로 토큰화 됨"
      ],
      "metadata": {
        "id": "XoUXcFEe6ni5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "sentences = sent_tokenize(raw_text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6UoUdyz5psQ",
        "outputId": "a1ec251a-3101-4e50-a74d-13e2cca942bb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어가 텍스트일 때만 할 수 있는 최대한의 전처리를 끝내놓아야 합니다."
      ],
      "metadata": {
        "id": "AXdveG676zbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "# stop_words 셋팅\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "    # 단어 토큰화\n",
        "    tokenized_sentence = word_tokenize(sentence)\n",
        "    result = []\n",
        "\n",
        "    for word in tokenized_sentence: \n",
        "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
        "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.\n",
        "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\n",
        "                result.append(word)\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = 0 \n",
        "                vocab[word] += 1\n",
        "    preprocessed_sentences.append(result) \n",
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfRprQUi6sDh",
        "outputId": "77e7a339-47b8-4ebd-da66-10b316d1f73a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 단어에 대한 빈도수가 기록\n",
        "print('단어 집합 :',vocab)\n",
        "# 단어를 키(key)로, 단어에 대한 빈도수가 값(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuS_z6hJ8JmZ",
        "outputId": "e0a8e960-6803-4d43-c6be-260669c93981"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'barber'라는 단어의 빈도수 출력\n",
        "print(vocab[\"barber\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS_B4_Fa9B8s",
        "outputId": "2f131ed8-40ff-49ab-fc7f-809a5b1d1535"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#빈도수가 높은 순서대로 정렬\n",
        "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
        "print(vocab_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie0APsxb9EUl",
        "outputId": "baeaf111-8f7b-42c4-dd94-fa83789259c7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 높은 빈도수를 가진 단어일수록 낮은 정수를 부여한다. 정수는 1부터 부여\n",
        "- 등장 빈도가 낮은 단어는 자연어 처리에서 의미를 가지지 않을 가능성이 높다.\n",
        "- 여기서는 빈도수가 1인 단어들은 전부 제외"
      ],
      "metadata": {
        "id": "SL09av3f9fP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab_sorted :\n",
        "    if frequency > 1 : # 빈도수가 작은 단어는 제외.\n",
        "        i = i + 1\n",
        "        word_to_index[word] = i\n",
        "\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAHdgUQA9aOK",
        "outputId": "d11c7792-689e-4474-e490-7ee857573d0b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#상위 5개 단어만 사용한다고 가정\n",
        "vocab_size = 5\n",
        "\n",
        "# 인덱스가 5 초과인 단어 제거\n",
        "words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
        "\n",
        "# 해당 단어에 대한 인덱스 정보를 삭제\n",
        "for w in words_frequency:\n",
        "    del word_to_index[w]\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgUjQ7s_-o8A",
        "outputId": "56f68f06-0246-4f23-9152-ce0d3b0a07e8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. 첫번째 문장은 ['barber', 'person'] => [1, 5]로 인코딩 <br>\n",
        " 2. 두번째 문장인 ['barber', 'good', 'person'] => word_to_index에는 존재하지 않는 단어인 'good'이라는 단어가 있다. <br>\n",
        "\n",
        "- **Out-Of-Vocabulary** : 단어 집합에 존재하지 않는 단어들이 생기는 상황을 Out-Of-Vocabulary(단어 집합에 없는 단어) 문제"
      ],
      "metadata": {
        "id": "QnFea6Fj_FFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'OOV': 6 단어 집합에 없는 단어들은 'OOV'의 인덱스로 인코딩\n",
        "word_to_index['OOV'] = len(word_to_index) + 1\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6HlN9OO-w-r",
        "outputId": "2b71c5cf-0ef3-4029-97e6-7b4578de60c5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sentences = []\n",
        "for sentence in preprocessed_sentences:\n",
        "    encoded_sentence = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.\n",
        "            encoded_sentence.append(word_to_index[word])\n",
        "        except KeyError:\n",
        "            # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\n",
        "            encoded_sentence.append(word_to_index['OOV'])\n",
        "    encoded_sentences.append(encoded_sentence)\n",
        "print(encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "surgTZMl_iRF",
        "outputId": "7a19d7f7-f438-4cf0-a43c-338278e55fb8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Counter 사용하기"
      ],
      "metadata": {
        "id": "JRWEl-rm_tyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "YHe0QOffCF2U"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZsBKeXgCHRY",
        "outputId": "88a1cabf-72f9-4ca1-e7e2-fa2c7d0ddd21"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 집합(vocabulary)을 만들기 위해서 sentences에서 문장의 경계인 [, ]를 제거하고 단어들을 하나의 리스트로 만듦"
      ],
      "metadata": {
        "id": "sQztavtLCK_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# words = np.hstack(preprocessed_sentences)으로도 수행 가능.\n",
        "all_words_list = sum(preprocessed_sentences, [])\n",
        "print(all_words_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opK00TkJCPix",
        "outputId": "f1e9d26f-c70b-486b-8c64-65fa9c626364"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counter() : 사용하면 중복을 제거하고 단어의 빈도수를 기록"
      ],
      "metadata": {
        "id": "V913kWcxD-t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이썬의 Counter 모듈을 이용하여 단어의 빈도수 카운트\n",
        "vocab = Counter(all_words_list)\n",
        "print(vocab)\n",
        "# 단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLGDv6AuD8US",
        "outputId": "37c1dc9c-14ff-424e-f346-c1c395b98223"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKevQSUXEDDg",
        "outputId": "009a99db-0967-4ec9-f7f5-d88071bc7a44"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈도수 상위 5개의 단어만 단어 집합으로 저장\n",
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0y5G4YvEPag",
        "outputId": "2d7ccfb5-d4c4-4330-89e9-473e6c90b54c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab :\n",
        "    i = i + 1\n",
        "    word_to_index[word] = i\n",
        "\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r0YHbjZEUQ7",
        "outputId": "db2aa888-4ad3-4b33-b228-a7d9183c4825"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 NLTK의 FreqDist 사용하기"
      ],
      "metadata": {
        "id": "6Vp2dduQEYZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK에서는 빈도수 계산 도구인 FreqDist()를 지원. 위에서 사용한 Counter()랑 같은 방법으로 사용"
      ],
      "metadata": {
        "id": "jzZOvs2TEeCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "owi_c2MtEgBk"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.hstack으로 문장 구분을 제거\n",
        "vocab = FreqDist(np.hstack(preprocessed_sentences))\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOpZ2SSxEhPf",
        "outputId": "3747befb-e3fe-4dc7-ef76-6fe2cf5458ca"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'barber': 8,\n",
              "          'crazy': 1,\n",
              "          'driving': 1,\n",
              "          'good': 1,\n",
              "          'huge': 5,\n",
              "          'keeping': 2,\n",
              "          'kept': 4,\n",
              "          'knew': 1,\n",
              "          'mountain': 1,\n",
              "          'person': 3,\n",
              "          'secret': 6,\n",
              "          'went': 1,\n",
              "          'word': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQbJGaV5EjMQ",
        "outputId": "93567eff-bcf5-48dd-fffb-e9538ec2c902"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLT_ImLYExSC",
        "outputId": "9c717512-b9ad-4d5e-c20e-d2b013968c2e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여\n",
        "# enumerate()를 사용하여 좀 더 짧은 코드로 인덱스를 부여\n",
        "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwkJ29JwE257",
        "outputId": "340fa2dc-56a6-4663-9840-2ecf4939f363"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4 enumerate 이해하기"
      ],
      "metadata": {
        "id": "4LrFzIDhE_XW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "enumerate()는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 함께 리턴"
      ],
      "metadata": {
        "id": "OC1P3c-MFDZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 리스트의 모든 토큰에 대해서 인덱스가 순차적으로 증가되며 부여\n",
        "test_input = ['a', 'b', 'c', 'd', 'e']\n",
        "for index, value in enumerate(test_input): # 입력의 순서대로 0부터 인덱스를 부여함.\n",
        "  print(\"value : {}, index: {}\".format(value, index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exSNk7b-FEEN",
        "outputId": "2fae50a5-9704-46b3-f862-b537e88135cc"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value : a, index: 0\n",
            "value : b, index: 1\n",
            "value : c, index: 2\n",
            "value : d, index: 3\n",
            "value : e, index: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 케라스(Keras)의 텍스트 전처리"
      ],
      "metadata": {
        "id": "rW8gx32WFUZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스(Keras)는 기본적인 전처리를 위한 도구들을 제공 <br>\n",
        "정수 인코딩을 위해서 케라스의 전처리 도구인 토크나이저를 사용"
      ],
      "metadata": {
        "id": "Zdkwb6N8FZQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "rVM_iFJcFdTT"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], \n",
        "                          ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], \n",
        "                          ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], \n",
        "                          ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "metadata": {
        "id": "y1H9klUZFeS1"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수 인코딩 작업\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
        "tokenizer.fit_on_texts(preprocessed_sentences) "
      ],
      "metadata": {
        "id": "4xbHZvBhFhm-"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈도수가 높은 순서대로 인덱스가 부여된 것을 확인\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adZ0202aF_CS",
        "outputId": "da5e0db9-d426-4500-bde3-c5b5c601a682"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLN8Gd6TGBak",
        "outputId": "71f783ae-a4de-49ca-df58-8125043a37d8"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# texts_to_sequences()는 입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcdXlhIfGGeW",
        "outputId": "14ec179c-3a5a-4604-98cb-f1c9d7815667"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스 토크나이저에서는 tokenizer = Tokenizer(num_words=숫자)와 같은 방법으로 빈도수가 높은 상위 몇 개의 단어만 사용하겠다고 지정할 수 있다."
      ],
      "metadata": {
        "id": "0k1uHwrqGTzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "# num_words는 숫자를 0부터 카운트\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "metadata": {
        "id": "EX3h6C8zGMQJ"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "실질적으로 숫자 0에 지정된 단어가 존재하지 않는데도 케라스 토크나이저가 숫자 0까지 단어 집합의 크기로 산정하는 이유는 자연어 처리에서 패딩(padding)이라는 작업 때문"
      ],
      "metadata": {
        "id": "DGf7UTYjGkGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8i8jgziGnrD",
        "outputId": "844742dc-a29b-4422-eb3f-a60abbf1d995"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vry8tAtrGq0-",
        "outputId": "0caba06c-ea04-4061-cbfb-ce7c404f6413"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제 적용은 texts_to_sequences를 사용할 때 적용"
      ],
      "metadata": {
        "id": "AnouaU0BGuC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV9oQSmKGx8s",
        "outputId": "32b51852-6f71-48d0-bb32-fe905135b227"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1번 단어부터 5번 단어까지만 보존되고 나머지 단어들은 제거된 것을 볼 수 있다."
      ],
      "metadata": {
        "id": "hRdKo1FwHADD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_index와 word_counts에서도 지정된 num_words만큼의 단어만 남기고 싶다면?"
      ],
      "metadata": {
        "id": "zb6a32K9HFPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "metadata": {
        "id": "6U_qjV-OHKeU"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "words_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1] \n",
        "\n",
        "# 인덱스가 5 초과인 단어 제거\n",
        "for word in words_frequency:\n",
        "    del tokenizer.word_index[word] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "    del tokenizer.word_counts[word] # 해당 단어에 대한 카운트 정보를 삭제\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWc_PKGhHLt6",
        "outputId": "02f9c7c6-c442-4e86-a3c6-184055442d59"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 Tokenizer의 인자 oov_token을 사용"
      ],
      "metadata": {
        "id": "5s4UcXzBHPkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "metadata": {
        "id": "55Zx559OHRD1"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eguHWQYnHTgT",
        "outputId": "dd7839ee-83da-45fd-f704-d520a27fa12e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 OOV의 인덱스 : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cB1gb8fHV0e",
        "outputId": "63e03334-8c75-4c76-91d1-7a4bd5c9a2b4"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "빈도수 상위 5개의 단어는 2 ~ 6까지의 인덱스를 가졌으며, 그 외 단어 집합에 없는 'good'과 같은 단어들은 전부 'OOV'의 인덱스인 1로 인코딩 됨"
      ],
      "metadata": {
        "id": "vh8-ZkgiHYsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07) 패딩(Padding)\n"
      ],
      "metadata": {
        "id": "iHj-GZoco6_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08) 원-핫 인코딩(One-Hot Encoding)"
      ],
      "metadata": {
        "id": "Z-KLWJlMo-TS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09) 데이터의 분리(Splitting Data)"
      ],
      "metadata": {
        "id": "oPIq_u1ipAd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10) 한국어 전처리 패키지(Text Preprocessing Tools for Korean Text)"
      ],
      "metadata": {
        "id": "q0cnqSNVpCUf"
      }
    }
  ]
}