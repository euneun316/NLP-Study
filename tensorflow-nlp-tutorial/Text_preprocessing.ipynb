{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/euneun316/NLP-Study/blob/main/Natural%20Language%20Processing/Text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRzw_LZl5Vwb"
   },
   "source": [
    "# 02.텍스트 전처리(Text preprocessing)\n",
    "- 풀고자 하는 문제의 용도에 맞게 텍스트를 사전에 처리하는 작업\n",
    "- 참조 Url : https://github.com/ukairia777/tensorflow-nlp-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shoCHgaGAXip"
   },
   "source": [
    "## 01) 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVcqYKDC1Tv0"
   },
   "source": [
    "주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83z1SE_j-hhB"
   },
   "source": [
    "### 1.단어 토큰화(Word Tokenization)\n",
    "- NLTK는 영어 코퍼스를 토큰화하기 위한 도구들을 제공\n",
    "- word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "se6WFb5o7uZ7"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5td_xf7w9327",
    "outputId": "ea115332-414d-4108-fc9c-ed5afd4e8e31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5gMGxHtf9waH",
    "outputId": "296e922c-0c80-4f2a-c13d-aa41ad2958ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화1 :',word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLflZzsK-Z3y"
   },
   "source": [
    "- wordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPQgsy7--Mx4",
    "outputId": "d2ec8538-fc85-4777-fc0f-a5c4e7a52c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화2 : ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화2 :',WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezfLaC3S-U1K"
   },
   "source": [
    "- 케라스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3t6FJ_q-Sls",
    "outputId": "659496aa-32f2-4999-8d07-e2f9de9054fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화3 : [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화3 :',text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsx9zCT4_M_i"
   },
   "source": [
    "### 2.표준 토큰화 예제\n",
    "- Penn Treebank *Tokenization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhDdhGJt_HF0",
    "outputId": "7f1eb83b-e6cd-4a70-97c5-ca575d4b065f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리뱅크 워드토크나이저 : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print('트리뱅크 워드토크나이저 :',tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsIVuPXR1biE"
   },
   "source": [
    "### 3.문장 토큰화(Sentence Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CViVyl5r_fW2"
   },
   "source": [
    "- NLTK에서는 영어 문장의 토큰화를 수행하는 : sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKlHywOv_kU5",
    "outputId": "a3bec875-3474-4d7e-c4b2-4d73e0db6903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print('문장 토큰화1 :',sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_b9TQv5_pG7"
   },
   "source": [
    "- 문장 중간에 마침표가 다수 등장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVmTB6kC_rNe",
    "outputId": "92a8a45e-8727-4376-e689-8fcf9ba21004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print('문장 토큰화2 :',sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C75QUDd3_4RX"
   },
   "source": [
    "- 박상길님이 개발한 KSS(Korean Sentence Splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efgoeOKj_uoz",
    "outputId": "f6e4befd-9680-46a4-83d3-60f68d06aa99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-3.4.tar.gz (42.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 42.4 MB 12.0 MB/s \n",
      "\u001b[?25hCollecting emoji\n",
      "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
      "\u001b[K     |████████████████████████████████| 174 kB 38.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from kss) (2019.12.20)\n",
      "Building wheels for collected packages: kss, emoji\n",
      "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kss: filename=kss-3.4-py3-none-any.whl size=42449209 sha256=e7fe29bb0ba38b0593943e5e0ad5458f62a9da0035afe2c5546f9238f60d6811\n",
      "  Stored in directory: /root/.cache/pip/wheels/06/8e/5b/305f0a804fba3943f353f1b0e3cb1fad39e4f5ae4893ea9590\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=bb7f6d70bdfeab9d42310655c4da835adf2118f6cf2e07a359e775c7a8764a19\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
      "Successfully built kss emoji\n",
      "Installing collected packages: emoji, kss\n",
      "Successfully installed emoji-1.6.3 kss-3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3x0MHBf6_zur",
    "outputId": "5f8f2943-1096-4f17-dba7-82115438a5fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
    "print('한국어 문장 토큰화 :',kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1UA4fYlANRq"
   },
   "source": [
    "### 4.NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPORpfWqBHeJ"
   },
   "source": [
    "- NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brW6VInjAvw-",
    "outputId": "c7fee608-7419-4586-d389-e1342dc52a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1I7wXuZlAr0h",
    "outputId": "52cf1789-0837-4b79-a61a-1b111caca932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('단어 토큰화 :',tokenized_sentence)\n",
    "print('품사 태깅 :',pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CuaBQSaA1vZ"
   },
   "source": [
    "- Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2E3J71JzA7IJ",
    "outputId": "b2a0fcd0-b3ea-4a74-94b5-f0de39a982de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4 MB 1.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
      "\u001b[K     |████████████████████████████████| 448 kB 48.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8lZvoiCA6PN",
    "outputId": "56215bd3-909a-43d0-a11e-b56b5ca20753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "print('OKT 형태소 분석 :',okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('OKT 품사 태깅 :',okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('OKT 명사 추출 :',okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hdjx-sPfBN0P"
   },
   "source": [
    "- 꼬꼬마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCfLw8LGBU_A",
    "outputId": "355a0ca5-87fa-4800-d954-6aeeae70f238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
      "꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
      "꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print('꼬꼬마 형태소 분석 :',kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('꼬꼬마 품사 태깅 :',kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('꼬꼬마 명사 추출 :',kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfmF_UI4B3C9"
   },
   "source": [
    "## 02) 정제(Cleaning) and 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtRWOW16C5pI"
   },
   "source": [
    "- 영어는 길이가 2~3 이하인 단어를 제거하는 것만으로도 크게 의미를 갖지 못하는 단어를 줄이는 효과를 갖고있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9UpvKV6B6Lt",
    "outputId": "f036f15a-90ba-4b21-f23f-a7934b3ba737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "\n",
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYaelMhVCM3e"
   },
   "source": [
    "## 03) 어간 추출(Stemming) and 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-Wr0end134L"
   },
   "source": [
    "눈으로 봤을 때는 서로 다른 단어들이지만, 하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이겠다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOCZ3SF1ELCH"
   },
   "source": [
    "### 1.표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmnJuaVh17QA"
   },
   "source": [
    "표제어(Lemma)는 한글로는 '표제어' 또는 '기본 사전형 단어' 정도의 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2Pb2vlzEF_S",
    "outputId": "d01a3dbb-d8ca-407f-de61-c99e84316bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0606hDbiEELz",
    "outputId": "31c1fe89-7576-4ba9-f974-a8c33cfd54c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print('표제어 추출 전 :',words)\n",
    "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxWJdTR3Eb7M"
   },
   "source": [
    "- WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "h5ud8DewEXQ1",
    "outputId": "f23798ce-d5d5-4988-f5c9-b07c2190e72a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "vCnHRnxrEejk",
    "outputId": "23cbb649-f003-4fa6-a120-45cf403d1b62"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "wR3aPYOqEgQm",
    "outputId": "ab7c7087-b524-4a5a-cfd3-4d3a7989d0d3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSjuUWMvEiRs"
   },
   "source": [
    "### 2.어간 추출(Stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVArjPb-2Bpp"
   },
   "source": [
    "어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K5FrSLNFgwK"
   },
   "source": [
    "- 포터 알고리즘(Porter Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYZzMmxIFYFe",
    "outputId": "d46852e1-2fe7-478a-cf88-d20aee083d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "print('어간 추출 전 :', tokenized_sentence)\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSZ63_A7FcKN",
    "outputId": "95926079-71f4-47b7-cc65-81f5c0d9a129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['formalize', 'allowance', 'electricical']\n",
      "어간 추출 후 : ['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words = ['formalize', 'allowance', 'electricical']\n",
    "\n",
    "print('어간 추출 전 :',words)\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_Xew6KnFoDD"
   },
   "source": [
    "- NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer) 알고리즘을 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acLUZvNeGFWk",
    "outputId": "f29b3f02-6052-46f6-93da-6aad6643b284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print('어간 추출 전 :', words)\n",
    "print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\n",
    "print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kudEfi2jGOVH"
   },
   "source": [
    "### 3.한국어에서의 어간 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orKsrV8bHiA6"
   },
   "source": [
    "## 04) 불용어(Stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzDUhMvl1yoi"
   },
   "source": [
    "데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fWs9-zONHjwX"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5USgMVK7HwD9"
   },
   "source": [
    "### 1.NLTK에서 불용어 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1P6UFPnSHzsU",
    "outputId": "8da1ae82-259d-4838-c21f-478cd7273aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5ya0wd4HxLC",
    "outputId": "6f8785b2-2355-41b1-d01e-4ae49b74403b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words_list = stopwords.words('english')\n",
    "print('불용어 개수 :', len(stop_words_list))\n",
    "print('불용어 10개 출력 :',stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgzxEwLdH5fX"
   },
   "source": [
    "### 2.NLTK를 통해서 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-B5yMHEH6M5",
    "outputId": "52c64d0e-706b-4f62-e6ef-3ec9daaada1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words: \n",
    "        result.append(word) \n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGeBAQIQICsa"
   },
   "source": [
    "### 3.한국어에서 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5rr1MZWIDxz",
    "outputId": "c2ba3e31-fe6a-46a2-e744-9c651f6e4f4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
      "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
    "\n",
    "stop_words = set(stop_words.split(' '))\n",
    "word_tokens = okt.morphs(example)\n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT3S2zN3IbuV"
   },
   "source": [
    "## 05) 정규 표현식(Regular Expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xB0nxL1lIn-0"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HCXsbi_I2vo"
   },
   "source": [
    "1) .기호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "A-ghdvNrIpji"
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"a.c\")\n",
    "r.search(\"kkk\") # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncTIDyD0Ivu3",
    "outputId": "4d905710-706c-4187-dbfe-61f525c95cdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Sqhpy8VI6gs"
   },
   "source": [
    "2) ?기호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "0dv3PCoyIy9d"
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"ab?c\")\n",
    "r.search(\"abbc\") # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqbgKPK7JB9q",
    "outputId": "069bc8ef-efd0-48f5-bc61-6edbd5ee1d1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HdlmfVqBJGLh",
    "outputId": "33c574a8-e761-4861-be42-9dd82ca03133"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXQkwt83JJIR"
   },
   "source": [
    "3) *기호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "esacixDvJQaW"
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"ab*c\")\n",
    "r.search(\"a\") # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrcjSOQHJSWq",
    "outputId": "7fe46234-fa6b-4d94-8dcb-30fe00edd2a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEBz48h-JUTl",
    "outputId": "bf7dfc33-2a44-4e08-98ce-136c6191483a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zGjitM4JVtl",
    "outputId": "590e55e8-2ef6-4567-f05a-42ce8b9b3988"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='abbbbc'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbbbc\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0_gAfU9JXRY"
   },
   "source": [
    "4) +기호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "khrV4fVcJcDs"
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"ab+c\")\n",
    "r.search(\"ac\") # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBySeBFNJdy8",
    "outputId": "2254a97f-8117-4b5a-8029-fdc2d7dab02b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxmyL876JfLG",
    "outputId": "6c938dbf-fd06-4cdd-8571-74520126d992"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='abbbbc'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbbbc\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4fPlUb5Jg2e"
   },
   "source": [
    "5) ^기호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "uZgU7VtGJl24"
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"^ab\")\n",
    "\n",
    "# 아무런 결과도 출력되지 않는다.\n",
    "r.search(\"bbc\")\n",
    "r.search(\"zab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6msQ4MXcJoBm",
    "outputId": "5cf5411e-8d06-4267-c7ca-a14900f3fbc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ab'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHb9gMUvJp4T"
   },
   "source": [
    "6) {숫자} 기호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "oT0F-GAJJvm6"
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"ab{2}c\")\n",
    "\n",
    "# 아무런 결과도 출력되지 않는다.\n",
    "r.search(\"ac\")\n",
    "r.search(\"abc\")\n",
    "r.search(\"abbbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFms3kdeJyPG",
    "outputId": "c74a6b4f-d03b-475c-ff75-758a4ef08489"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbc'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjk7fWXoJ0Nf"
   },
   "source": [
    "7) {숫자1, 숫자2} 기호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "FuXXJBKPJ673"
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"ab{2,8}c\")\n",
    "\n",
    "# 아무런 결과도 출력되지 않는다.\n",
    "r.search(\"ac\")\n",
    "r.search(\"abc\")\n",
    "r.search(\"abbbbbbbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rpOgZhQJ-lr",
    "outputId": "67e2251e-6d91-4738-e494-7f6a54c1a395"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbc'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc2kmUW-J_2j",
    "outputId": "ec5f5d63-5c13-426e-a975-15184691e303"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 10), match='abbbbbbbbc'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbbbbbbbc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0ROFecPKBeM"
   },
   "source": [
    "8) {숫자,} 기호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "xbG3qe2ZKImk"
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"a{2,}bc\")\n",
    "\n",
    "# 아무런 결과도 출력되지 않는다.\n",
    "r.search(\"bc\")\n",
    "r.search(\"aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KF34kWGoKN6C",
    "outputId": "ca499b16-3da0-485e-f85f-94ced85f6a8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='aabc'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vGvr8oWKQZa",
    "outputId": "562e5614-6f03-4ea1-87c2-3ce9c0528650"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 10), match='aaaaaaaabc'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aaaaaaaabc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDIKZRnrKR1Y"
   },
   "source": [
    "9) [ ] 기호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Z0AE-dcfKX99"
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"[abc]\") # [abc]는 [a-c]와 같다.\n",
    "r.search(\"zzz\") # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe9rcYDJKZcL",
    "outputId": "dc176a23-c914-4e2f-f1b5-462a20581183"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zlm7D6RQKb4X",
    "outputId": "0d359b52-d9b6-41df-e299-57c091122e46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aaaaaaa\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JcWkGbCLKdE-",
    "outputId": "a541b07c-0433-4813-8350-d1c9407a01da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='b'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"baac\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "fL4FSugrKep8"
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"[a-z]\")\n",
    "\n",
    "# 아무런 결과도 출력되지 않는다.\n",
    "r.search(\"AAA\")\n",
    "r.search(\"111\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1cw8gKAKf9T",
    "outputId": "ee889a4d-624d-4f8e-d49c-dcff33b77936"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aBC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dB_eIS5zKhe3"
   },
   "source": [
    "10) [^문자] 기호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "neFZV7YUKmrU"
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"[^abc]\")\n",
    "\n",
    "# 아무런 결과도 출력되지 않는다.\n",
    "r.search(\"a\")\n",
    "r.search(\"ab\") \n",
    "r.search(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFy326iaKoMy",
    "outputId": "eca7f486-721e-439b-9ff6-0c2b5e8fa95b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='d'>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgDxzBucKpv2",
    "outputId": "9ed4ddce-d2de-46a5-f318-0e3bd1d6a5b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='1'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"1\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ8YXS97KriF"
   },
   "source": [
    "### 3.정규 표현식 모듈 함수 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0Opb3dQK936"
   },
   "source": [
    "(1) re.match() 와 re.search()의 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "ueZYrxmlKsue"
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"ab.\")\n",
    "r.match(\"kkkabc\") # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdoNI5N0K5MB",
    "outputId": "d4ab8ef6-2935-40bd-a99c-64ccdbaf2f85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 6), match='abc'>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"kkkabc\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3et8rS_QK77V",
    "outputId": "de5a5e48-b2dd-41cf-d734-4e5a5252668f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.match(\"abckkk\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnDfrlGtLBqq"
   },
   "source": [
    "(2) re.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tgeITzXOLFe2",
    "outputId": "6a1a54df-9ca9-429f-9d40-89f7410588cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 공백 기준 분리\n",
    "text = \"사과 딸기 수박 메론 바나나\"\n",
    "re.split(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLXTHLpsLIPd",
    "outputId": "d4d551ea-596c-417c-914d-c46fbdac6015"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄바꿈 기준 분리\n",
    "text = \"\"\"사과\n",
    "딸기\n",
    "수박\n",
    "메론\n",
    "바나나\"\"\"\n",
    "\n",
    "re.split(\"\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLi4NYXSLLtg",
    "outputId": "e06fc192-d148-472d-acd8-407a120bc016"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '+'를 기준으로 분리\n",
    "text = \"사과+딸기+수박+메론+바나나\"\n",
    "\n",
    "re.split(\"\\+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLoWbvwYLNwD"
   },
   "source": [
    "(3) re.findall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHdFi8CWLSZE",
    "outputId": "a2c6b849-256b-4598-ce17-898a5a68e6ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '1234', '30']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"이름 : 김철수\n",
    "전화번호 : 010 - 1234 - 1234\n",
    "나이 : 30\n",
    "성별 : 남\"\"\"\n",
    "\n",
    "re.findall(\"\\d+\", text) #숫자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLBbFz98LZ_j",
    "outputId": "594309ed-0850-496f-ca56-8a584b53de6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"\\d+\", \"문자열입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yohp5G0aLekC"
   },
   "source": [
    "(4) re.sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rb7pQ_lcLhwu",
    "outputId": "afdf5953-86bc-49dd-bdd8-4d47e315d028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern \n"
     ]
    }
   ],
   "source": [
    "text = \"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "\n",
    "preprocessed_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGiaooi2LuHc"
   },
   "source": [
    "### 4.정규 표현식 텍스트 전처리 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "ytusGjjgLwPn"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"100 John    PROF\n",
    "101 James   STUD\n",
    "102 Mac   STUD\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1ndjq_WLy-z",
    "outputId": "13efde37-79fc-4b5b-b1df-daa4b5c2eafd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MlcBVabqL2q2",
    "outputId": "96542002-202d-45e9-a00b-79c1b19c0aca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', '101', '102']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d+',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nj9IJV3vL4la",
    "outputId": "91047a86-9f5f-40ee-9693-3948538a5e27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oALsQhBoL6MB",
    "outputId": "002d648a-b0a8-44af-c036-07a934bd40ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROF', 'STUD', 'STUD']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]{4}',text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MU12BhjXL-BR",
    "outputId": "4d7f1239-122a-4534-f403-09b20aa5a7de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'James', 'Mac']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z][a-z]+',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5TOJBteMAKw"
   },
   "source": [
    "### 5.정규 표현식을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3G4rDPMiMNP_",
    "outputId": "172bd460-831c-406f-fb51-92674e50b94e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"\n",
    "\n",
    "tokenizer1 = RegexpTokenizer(\"[\\w]+\")\n",
    "tokenizer2 = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "\n",
    "print(tokenizer1.tokenize(text))\n",
    "print(tokenizer2.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAdOYw1xo1pb"
   },
   "source": [
    "## 06) 정수 인코딩(Integer Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djwQ3JW7wSpE"
   },
   "source": [
    "텍스트를 숫자로 바꾸는 여러가지 기법들이 있습니다.<br>\n",
    "각 단어를 고유한 정수에 맵핑(mapping)시키는 전처리 작업이 필요할 때가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7c3mO6r3E8R"
   },
   "source": [
    "### 1.정수 인코딩(Integer Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOxYGYbX3IQK"
   },
   "source": [
    "단어에 정수를 부여하는 방법 중 하나로 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APW0NZL75efQ"
   },
   "source": [
    "#### 1.1 dictionary 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "jvidcnqS5leg"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "kBLnSXg45nbV"
   },
   "outputs": [],
   "source": [
    "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoUXcFEe6ni5"
   },
   "source": [
    "기존의 텍스트 데이터가 문장 단위로 토큰화 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6UoUdyz5psQ",
    "outputId": "0079b580-abd8-4544-dab2-aa2f64906d9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "sentences = sent_tokenize(raw_text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXdveG676zbB"
   },
   "source": [
    "단어가 텍스트일 때만 할 수 있는 최대한의 전처리를 끝내놓아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfRprQUi6sDh",
    "outputId": "8c8e715b-e4b8-45d6-813c-57019acd4453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "# stop_words 셋팅\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    # 단어 토큰화\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized_sentence: \n",
    "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
    "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.\n",
    "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    preprocessed_sentences.append(result) \n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuS_z6hJ8JmZ",
    "outputId": "fe18f982-65d3-4741-97c8-7fb5d63196dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 : {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "# 각 단어에 대한 빈도수가 기록\n",
    "print('단어 집합 :',vocab)\n",
    "# 단어를 키(key)로, 단어에 대한 빈도수가 값(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SS_B4_Fa9B8s",
    "outputId": "6aba2556-ece9-4aed-e434-f690d36bc627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# 'barber'라는 단어의 빈도수 출력\n",
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ie0APsxb9EUl",
    "outputId": "fb2435a6-113e-4d2d-c46a-38dfb51b5823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "#빈도수가 높은 순서대로 정렬\n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SL09av3f9fP8"
   },
   "source": [
    "- 높은 빈도수를 가진 단어일수록 낮은 정수를 부여한다. 정수는 1부터 부여\n",
    "- 등장 빈도가 낮은 단어는 자연어 처리에서 의미를 가지지 않을 가능성이 높다.\n",
    "- 여기서는 빈도수가 1인 단어들은 전부 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAHdgUQA9aOK",
    "outputId": "7d044396-010b-4662-b481-cd744254d897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab_sorted :\n",
    "    if frequency > 1 : # 빈도수가 작은 단어는 제외.\n",
    "        i = i + 1\n",
    "        word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgUjQ7s_-o8A",
    "outputId": "23a61790-ba2c-4f03-e4e9-a686625fb048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "#상위 5개 단어만 사용한다고 가정\n",
    "vocab_size = 5\n",
    "\n",
    "# 인덱스가 5 초과인 단어 제거\n",
    "words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
    "\n",
    "# 해당 단어에 대한 인덱스 정보를 삭제\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w]\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnFea6Fj_FFN"
   },
   "source": [
    " 1. 첫번째 문장은 ['barber', 'person'] => [1, 5]로 인코딩 <br>\n",
    " 2. 두번째 문장인 ['barber', 'good', 'person'] => word_to_index에는 존재하지 않는 단어인 'good'이라는 단어가 있다. <br>\n",
    "\n",
    "- **Out-Of-Vocabulary** : 단어 집합에 존재하지 않는 단어들이 생기는 상황을 Out-Of-Vocabulary(단어 집합에 없는 단어) 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6HlN9OO-w-r",
    "outputId": "1307b7eb-4649-4ed1-e26f-5070db34916b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
     ]
    }
   ],
   "source": [
    "# 'OOV': 6 단어 집합에 없는 단어들은 'OOV'의 인덱스로 인코딩\n",
    "word_to_index['OOV'] = len(word_to_index) + 1\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "surgTZMl_iRF",
    "outputId": "9666b173-dd03-49dc-ca7e-86e851e1ab11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = []\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.\n",
    "            encoded_sentence.append(word_to_index[word])\n",
    "        except KeyError:\n",
    "            # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\n",
    "            encoded_sentence.append(word_to_index['OOV'])\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRWEl-rm_tyV"
   },
   "source": [
    "#### 1.2 Counter 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "YHe0QOffCF2U"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZsBKeXgCHRY",
    "outputId": "aa1d8a2d-4fa0-4d16-fe74-c35416a4b52b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQztavtLCK_6"
   },
   "source": [
    "단어 집합(vocabulary)을 만들기 위해서 sentences에서 문장의 경계인 [, ]를 제거하고 단어들을 하나의 리스트로 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opK00TkJCPix",
    "outputId": "3322299a-64c9-4538-99f9-ee5a59616547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# words = np.hstack(preprocessed_sentences)으로도 수행 가능.\n",
    "all_words_list = sum(preprocessed_sentences, [])\n",
    "print(all_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V913kWcxD-t0"
   },
   "source": [
    "Counter() : 사용하면 중복을 제거하고 단어의 빈도수를 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLGDv6AuD8US",
    "outputId": "b7119560-6498-4e9b-a8ea-fe95aa378f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "# 파이썬의 Counter 모듈을 이용하여 단어의 빈도수 카운트\n",
    "vocab = Counter(all_words_list)\n",
    "print(vocab)\n",
    "# 단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKevQSUXEDDg",
    "outputId": "29d9f4b2-f3ec-45a1-91f1-459e958e9719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0y5G4YvEPag",
    "outputId": "abecb69e-893b-4b67-e066-2c858d194e53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도수 상위 5개의 단어만 단어 집합으로 저장\n",
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9r0YHbjZEUQ7",
    "outputId": "070ecc88-0e12-42b3-8eac-c64e80b203cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab :\n",
    "    i = i + 1\n",
    "    word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Vp2dduQEYZr"
   },
   "source": [
    "#### 1.3 NLTK의 FreqDist 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzZOvs2TEeCB"
   },
   "source": [
    "NLTK에서는 빈도수 계산 도구인 FreqDist()를 지원. 위에서 사용한 Counter()랑 같은 방법으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "owi_c2MtEgBk"
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOpZ2SSxEhPf",
    "outputId": "e5abfe59-50f6-497b-9e7c-fa23e1dc7457"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'barber': 8,\n",
       "          'crazy': 1,\n",
       "          'driving': 1,\n",
       "          'good': 1,\n",
       "          'huge': 5,\n",
       "          'keeping': 2,\n",
       "          'kept': 4,\n",
       "          'knew': 1,\n",
       "          'mountain': 1,\n",
       "          'person': 3,\n",
       "          'secret': 6,\n",
       "          'went': 1,\n",
       "          'word': 2})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.hstack으로 문장 구분을 제거\n",
    "vocab = FreqDist(np.hstack(preprocessed_sentences))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQbJGaV5EjMQ",
    "outputId": "1a97f58d-ce9d-4cda-c0db-d91c1a35e5c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLT_ImLYExSC",
    "outputId": "93625cd2-ead4-49a1-e46d-aa79ba4abb4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwkJ29JwE257",
    "outputId": "b80d7646-f37b-4601-c6f0-1e01a36c5069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여\n",
    "# enumerate()를 사용하여 좀 더 짧은 코드로 인덱스를 부여\n",
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LrFzIDhE_XW"
   },
   "source": [
    "#### 1.4 enumerate 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC1P3c-MFDZj"
   },
   "source": [
    "enumerate()는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 함께 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exSNk7b-FEEN",
    "outputId": "0cd54376-a205-49a3-e7ca-3b7fe015c830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : a, index: 0\n",
      "value : b, index: 1\n",
      "value : c, index: 2\n",
      "value : d, index: 3\n",
      "value : e, index: 4\n"
     ]
    }
   ],
   "source": [
    "# 리스트의 모든 토큰에 대해서 인덱스가 순차적으로 증가되며 부여\n",
    "test_input = ['a', 'b', 'c', 'd', 'e']\n",
    "for index, value in enumerate(test_input): # 입력의 순서대로 0부터 인덱스를 부여함.\n",
    "  print(\"value : {}, index: {}\".format(value, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rW8gx32WFUZY"
   },
   "source": [
    "### 2.케라스(Keras)의 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdkwb6N8FZQr"
   },
   "source": [
    "케라스(Keras)는 기본적인 전처리를 위한 도구들을 제공 <br>\n",
    "정수 인코딩을 위해서 케라스의 전처리 도구인 토크나이저를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "rVM_iFJcFdTT"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "y1H9klUZFeS1"
   },
   "outputs": [],
   "source": [
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], \n",
    "                          ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], \n",
    "                          ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], \n",
    "                          ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "4xbHZvBhFhm-"
   },
   "outputs": [],
   "source": [
    "# 정수 인코딩 작업\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
    "tokenizer.fit_on_texts(preprocessed_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adZ0202aF_CS",
    "outputId": "c5e94712-cf51-4086-bbf6-375973b8f379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "# 빈도수가 높은 순서대로 인덱스가 부여된 것을 확인\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLN8Gd6TGBak",
    "outputId": "22811f94-402b-497d-a127-b1fd26987969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WcdXlhIfGGeW",
    "outputId": "206195ac-41be-48be-a8a1-0b248300e97d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "# texts_to_sequences()는 입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0k1uHwrqGTzw"
   },
   "source": [
    "케라스 토크나이저에서는 tokenizer = Tokenizer(num_words=숫자)와 같은 방법으로 빈도수가 높은 상위 몇 개의 단어만 사용하겠다고 지정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "EX3h6C8zGMQJ"
   },
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "# num_words는 숫자를 0부터 카운트\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGf7UTYjGkGx"
   },
   "source": [
    "실질적으로 숫자 0에 지정된 단어가 존재하지 않는데도 케라스 토크나이저가 숫자 0까지 단어 집합의 크기로 산정하는 이유는 자연어 처리에서 패딩(padding)이라는 작업 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8i8jgziGnrD",
    "outputId": "8a7ccd4b-00eb-4b72-bf44-2524923bf6e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vry8tAtrGq0-",
    "outputId": "1a6d6f2a-7974-4281-f7f8-26a57a4c65a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnouaU0BGuC2"
   },
   "source": [
    "실제 적용은 texts_to_sequences를 사용할 때 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DV9oQSmKGx8s",
    "outputId": "ee95b0c4-fcca-43b7-81e0-99da42b4a8d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRdKo1FwHADD"
   },
   "source": [
    "1번 단어부터 5번 단어까지만 보존되고 나머지 단어들은 제거된 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zb6a32K9HFPa"
   },
   "source": [
    "word_index와 word_counts에서도 지정된 num_words만큼의 단어만 남기고 싶다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "6U_qjV-OHKeU"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWc_PKGhHLt6",
    "outputId": "dd459f20-d193-45b4-9762-34673090505b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "words_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1] \n",
    "\n",
    "# 인덱스가 5 초과인 단어 제거\n",
    "for word in words_frequency:\n",
    "    del tokenizer.word_index[word] # 해당 단어에 대한 인덱스 정보를 삭제\n",
    "    del tokenizer.word_counts[word] # 해당 단어에 대한 카운트 정보를 삭제\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5s4UcXzBHPkf"
   },
   "source": [
    " 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 Tokenizer의 인자 oov_token을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "55Zx559OHRD1"
   },
   "outputs": [],
   "source": [
    "# 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eguHWQYnHTgT",
    "outputId": "8088e609-2855-4fa1-9576-7587ef26d0cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 OOV의 인덱스 : 1\n"
     ]
    }
   ],
   "source": [
    "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cB1gb8fHV0e",
    "outputId": "4aa16719-96b9-4932-b08e-b8782ec97271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vh8-ZkgiHYsk"
   },
   "source": [
    "빈도수 상위 5개의 단어는 2 ~ 6까지의 인덱스를 가졌으며, 그 외 단어 집합에 없는 'good'과 같은 단어들은 전부 'OOV'의 인덱스인 1로 인코딩 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHj-GZoco6_b"
   },
   "source": [
    "## 07) 패딩(Padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COxxP6i_QC9t"
   },
   "source": [
    "자연어 처리를 하다보면 각 문장(또는 문서)은 서로 길이가 다를 수 있다. <br>\n",
    "기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리 <br>\n",
    "병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QoPP-AGRSzO"
   },
   "source": [
    "### 1.Numpy로 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "bwTOvVpdQl-t"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "sw4nfqjoQ5VP"
   },
   "outputs": [],
   "source": [
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3A_x0GPGTir5",
    "outputId": "2fc79070-c7fa-485f-b561-4c3d969ecf8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩을 수행\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-aRANlzToyV",
    "outputId": "fb867131-e973-42f8-f29d-4c00babaf8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이 : 7\n"
     ]
    }
   ],
   "source": [
    "# 가장 길이가 긴 문장의 길이를 계산\n",
    "max_len = max(len(item) for item in encoded)\n",
    "print('최대 길이 :',max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBOhqktGTsWu"
   },
   "source": [
    "- 모든 문장의 길이를 7로 맞춰준다. \n",
    "- 이때 가상의 단어 'PAD'를 사용.\n",
    "- 이 단어는 0번 단어라고 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KBJcyGdT2PN",
    "outputId": "8560073a-b05c-42ef-f1c5-ba8de21ea8bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0,  0,  0]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sentence in encoded:\n",
    "    while len(sentence) < max_len:\n",
    "        sentence.append(0)\n",
    "\n",
    "padded_np = np.array(encoded)\n",
    "padded_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWiM7wHKVcI9"
   },
   "source": [
    "- 기계는 이들을 하나의 행렬로 보고, 병렬 처리를 할 수 있다. \n",
    "- 0번 단어는 사실 아무런 의미도 없는 단어이기 때문에 자연어 처리하는 과정에서 기계는 0번 단어를 무시한다.\n",
    "- 데이터에 특정 값을 채워서 데이터의 크기(shape)를 조정하는 것을 패딩(padding)이라고 한다.\n",
    "- 숫자 0을 사용하고 있다면 제로 패딩(zero padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17E4aTBmVwMA"
   },
   "source": [
    "### 2.케라스 전처리 도구로 패딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "KyODI91vV0sz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZrJmhJ7V407",
    "outputId": "be078135-8e33-460c-d0c1-f82fd9772676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvFKdjUpV_u3",
    "outputId": "4a88d98d-6b3d-49e1-e5a0-552909466333"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
       "       [ 0,  0,  0,  0,  1,  8,  5],\n",
       "       [ 0,  0,  0,  0,  1,  3,  5],\n",
       "       [ 0,  0,  0,  0,  0,  9,  2],\n",
       "       [ 0,  0,  0,  2,  4,  3,  2],\n",
       "       [ 0,  0,  0,  0,  0,  3,  2],\n",
       "       [ 0,  0,  0,  0,  1,  4,  6],\n",
       "       [ 0,  0,  0,  0,  1,  4,  6],\n",
       "       [ 0,  0,  0,  0,  1,  4,  2],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 0,  0,  0,  1, 12,  3, 13]], dtype=int32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras의 pad_sequences\n",
    "padded = pad_sequences(encoded)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RA5UVj2RWZc-"
   },
   "source": [
    "- pad_sequences는 기본적으로 문서의 뒤에 0을 채우는 것이 아니라 앞에 0으로 채운다.\n",
    "- 뒤에 0을 채우고 싶다면 인자로 padding='post'를 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnLwBjcoWg_R",
    "outputId": "df32377a-c1d4-4aac-ae24-42695f7f4715"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded, padding='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wfp_X3YTWkPq",
    "outputId": "d7253275-6665-4ab5-c4a6-5c0643690b2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy를 이용하여 패딩을 했을 때와 결과가 동일\n",
    "(padded == padded_np).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-3lMc6eWrnm"
   },
   "source": [
    "- 길이에 제한을 두고 패딩할 수 있다.\n",
    "- max_len의 인자로 정수를 주면, 해당 정수로 모든 문서의 길이를 동일하게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSZQcQNwWxT7",
    "outputId": "09413f55-892d-40bb-c01c-25fb1031d5a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0],\n",
       "       [ 3,  2,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0],\n",
       "       [ 3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0]], dtype=int32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5보다 짧은 문서들은 0으로 패딩되고, 기존에 5보다 길었다면 데이터가 손실\n",
    "padded = pad_sequences(encoded, padding='post', maxlen=5)\n",
    "padded\n",
    "# [ 7,  7,  3,  2, 10,  1, 11] ->  [ 3,  2, 10,  1, 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF9sENaaW7Jk"
   },
   "source": [
    "- 데이터가 손실될 경우에 앞의 단어가 아니라 뒤의 단어가 삭제되도록 하고싶다면 truncating이라는 인자를 사용한다.\n",
    "- truncating='post'를 사용할 경우 뒤의 단어가 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_caQ362dXAbI",
    "outputId": "6751568f-7650-421f-8504-68b14960ec54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0],\n",
       "       [ 3,  2,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10],\n",
       "       [ 1, 12,  3, 13,  0]], dtype=int32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)\n",
    "padded\n",
    "# [ 7,  7,  3,  2, 10,  1, 11] ->  [ 7,  7,  3,  2, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI3no5-2XiQi"
   },
   "source": [
    "숫자 0이 아니라 다른 숫자를 패딩을 위한 숫자로 사용하고 싶다면 이 또한 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZURuD5WXppG",
    "outputId": "a267b096-fb1f-4ae4-a14b-35bfd1c3e6d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "last_value = len(tokenizer.word_index) + 1 # 단어 집합의 크기보다 1 큰 숫자를 사용\n",
    "print(last_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_tU9uyQXuO-",
    "outputId": "9786c705-c7eb-4b1a-8b48-8fb7075a91c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5, 14, 14, 14, 14, 14],\n",
       "       [ 1,  8,  5, 14, 14, 14, 14],\n",
       "       [ 1,  3,  5, 14, 14, 14, 14],\n",
       "       [ 9,  2, 14, 14, 14, 14, 14],\n",
       "       [ 2,  4,  3,  2, 14, 14, 14],\n",
       "       [ 3,  2, 14, 14, 14, 14, 14],\n",
       "       [ 1,  4,  6, 14, 14, 14, 14],\n",
       "       [ 1,  4,  6, 14, 14, 14, 14],\n",
       "       [ 1,  4,  2, 14, 14, 14, 14],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13, 14, 14, 14]], dtype=int32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded, padding='post', value=last_value)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-KLWJlMo-TS"
   },
   "source": [
    "## 08) 원-핫 인코딩(One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cK505vRGmG1G"
   },
   "source": [
    "자연어 처리에서는 문자를 숫자로 바꾸는 여러가지 기법이 있는데, 원-핫 인코딩(One-Hot Encoding)은 그 많은 기법 중에서 단어를 표현하는 가장 기본적인 표현 방법\n",
    "\n",
    "**단어 집합(vocabulary)**\n",
    "- 서로 다른 단어들의 집합\n",
    "- book과 books와 같이 단어의 변형 형태도 다른 단어로 간주\n",
    "- 텍스트의 모든 단어를 중복을 허용하지 않고 모아둠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfhZRWEomyOx"
   },
   "source": [
    "### 1.원-핫 인코딩(One-Hot Encoding)이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q23L3IAWm2jN"
   },
   "source": [
    "단어 집합의 크기를 벡터의 차원으로 하고, <br>\n",
    "표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, <br>\n",
    "다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식 <br>\n",
    "이렇게 표현된 벡터를 _원-핫 벡터(One-Hot vector)_ 라고 한다.\n",
    "1. 정수 인코딩을 수행. 각 단어에 고유한 정수를 부여\n",
    "2. 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ol4vMKCnuzy",
    "outputId": "65ccba92-0431-4104-e572-ea33d0fe7e37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n"
     ]
    }
   ],
   "source": [
    "# Okt 형태소 분석기를 통해서 문장에 대해서 토큰화를 수행\n",
    "from konlpy.tag import Okt  \n",
    "\n",
    "okt = Okt()  \n",
    "tokens = okt.morphs(\"나는 자연어 처리를 배운다\")  \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjvlw0WIn0Ai",
    "outputId": "da7674f5-37b2-4513-9a28-692765c4ef37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 : {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    }
   ],
   "source": [
    "# 각 토큰에 대해서 고유한 정수를 부여\n",
    "# 빈도수 순으로 단어를 정렬하여 정수를 부여하는 경우가 많음\n",
    "word_to_index = {word : index for index, word in enumerate(tokens)}\n",
    "print('단어 집합 :',word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "9gJe2D27oFAL"
   },
   "outputs": [],
   "source": [
    "# 토큰을 입력하면 해당 토큰에 대한 원-핫 벡터를 만들어내는 함수\n",
    "def one_hot_encoding(word, word_to_index):\n",
    "  one_hot_vector = [0]*(len(word_to_index))\n",
    "  index = word_to_index[word]\n",
    "  one_hot_vector[index] = 1\n",
    "  return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wksWvvUZoG4w",
    "outputId": "940defb9-a4f4-4a8b-a153-b54404c53aed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"자연어\", word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p56km9-3pJjb"
   },
   "source": [
    "### 2.케라스(Keras)를 이용한 원-핫 인코딩(One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poTUIp8ApL9E"
   },
   "source": [
    "케라스는 원-핫 인코딩을 수행하는 유용한 도구 to_categorical()를 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "0eH8DSfEpO-o"
   },
   "outputs": [],
   "source": [
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dG3bmS3LpU5C",
    "outputId": "46ec76f6-09ae-4db6-bf81-08d1347d2079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 : {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "print('단어 집합 :',tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYxQ7Z1qj9iN",
    "outputId": "ea91e8ba-55cd-4d41-dca1-4027b09dafbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 1, 6, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "# texts_to_sequences()를 통해서 이를 정수 시퀀스로 변환가능\n",
    "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QdrDkAomkHGg",
    "outputId": "0e3f175e-f08a-4654-f8db-fe7c68697384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 원-핫 인코딩\n",
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrwR-nR4kOSg"
   },
   "source": [
    "###3.원-핫 인코딩(One-Hot Encoding)의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0MfsuZ3kTF1"
   },
   "source": [
    "1. 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점\n",
    "  - 벡터의 차원이 늘어난다\n",
    "  - 단어 집합의 크기 = 벡터의 차원 수\n",
    "2. 단어의 유사도를 표현하지 못한다는 단점\n",
    "\n",
    "=> 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법 사용\n",
    "1. 카운트 기반의 벡터화 방법인 LSA(잠재 의미 분석), HAL \n",
    "2. 측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText\n",
    "3. 두 가지 방법을 모두 사용하는 방법으로 GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPIq_u1ipAd0"
   },
   "source": [
    "## 09) 데이터의 분리(Splitting Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ULHAUQ3lHHn"
   },
   "source": [
    "지도 학습(Supervised Learning) <br>\n",
    "지도 학습을 위한 데이터 분리 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "ERPgaktbl2Ae"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpsLBNn-l3FO"
   },
   "source": [
    "### 1.지도 학습(Supervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEBxokdSmD6A"
   },
   "source": [
    "- 정답이 무엇인지 맞춰 하는 '문제'에 해당되는 데이터\n",
    "- 레이블이라고 부르는 '정답'이 적혀있는 데이터\n",
    "\n",
    "><훈련 데이터> <br>\n",
    ">X_train : 문제지 데이터<br>\n",
    ">y_train : 문제지에 대한 정답 데이터.\n",
    "\n",
    "><테스트 데이터> <br>\n",
    ">X_test : 시험지 데이터.<br>\n",
    ">y_test : 시험지에 대한 정답 데이터.\n",
    "\n",
    ">정확도(Accuracy) : 기계가 정답을 얼마나 맞췄는지를 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ch1F3WWGm5nP"
   },
   "source": [
    "### 2.X와 y분리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQj47R7LnBY_"
   },
   "source": [
    "####2.1 zip 함수를 이용하여 분리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETPeW89RnIQH"
   },
   "source": [
    "zip()함수 : 동일한 개수를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소들끼리 묶어주는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5gkDjQfUnNmr",
    "outputId": "3fea527b-f701-4658-aa7b-ab6a3bdc6d3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 데이터 : ('a', 'b', 'c')\n",
      "y 데이터 : (1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "X, y = zip(['a', 1], ['b', 2], ['c', 3])\n",
    "print('X 데이터 :',X)\n",
    "print('y 데이터 :',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zuvCe68pnPrt",
    "outputId": "c77d498a-3479-49df-ca41-6c50e61abcfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 데이터 : ('a', 'b', 'c')\n",
      "y 데이터 : (1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 리스트의 리스트 또는 행렬 또는 뒤에서 배울 개념인 2D 텐서.\n",
    "sequences = [['a', 1], ['b', 2], ['c', 3]]\n",
    "X, y = zip(*sequences)\n",
    "print('X 데이터 :',X)\n",
    "print('y 데이터 :',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGWz732znjnZ"
   },
   "source": [
    "####2.2 데이터프레임을 이용하여 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "OW6NTf3qnnOW",
    "outputId": "8bd074a5-9c1e-45f6-b934-ca71ca55fc68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-412ccfd3-be4c-4ef1-9b9d-d5ac4386cb83\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>메일 본문</th>\n",
       "      <th>스팸 메일 유무</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>당신에게 드리는 마지막 혜택!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>내일 뵐 수 있을지 확인 부탁드...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>도연씨. 잘 지내시죠? 오랜만입...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(광고) AI로 주가를 예측할 수 있다!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-412ccfd3-be4c-4ef1-9b9d-d5ac4386cb83')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-412ccfd3-be4c-4ef1-9b9d-d5ac4386cb83 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-412ccfd3-be4c-4ef1-9b9d-d5ac4386cb83');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                    메일 본문  스팸 메일 유무\n",
       "0        당신에게 드리는 마지막 혜택!         1\n",
       "1    내일 뵐 수 있을지 확인 부탁드...         0\n",
       "2    도연씨. 잘 지내시죠? 오랜만입...         0\n",
       "3  (광고) AI로 주가를 예측할 수 있다!         1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [['당신에게 드리는 마지막 혜택!', 1],\n",
    "['내일 뵐 수 있을지 확인 부탁드...', 0],\n",
    "['도연씨. 잘 지내시죠? 오랜만입...', 0],\n",
    "['(광고) AI로 주가를 예측할 수 있다!', 1]]\n",
    "columns = ['메일 본문', '스팸 메일 유무']\n",
    "\n",
    "df = pd.DataFrame(values, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "YcW63kTRns1E"
   },
   "outputs": [],
   "source": [
    "X = df['메일 본문']\n",
    "y = df['스팸 메일 유무']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Frg2UlYanuQx",
    "outputId": "291c1839-d78e-4e2d-a718-fed6f061601a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 데이터 : ['당신에게 드리는 마지막 혜택!', '내일 뵐 수 있을지 확인 부탁드...', '도연씨. 잘 지내시죠? 오랜만입...', '(광고) AI로 주가를 예측할 수 있다!']\n",
      "y 데이터 : [1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print('X 데이터 :',X.to_list())\n",
    "print('y 데이터 :',y.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoASuMMknwbx"
   },
   "source": [
    "#### 2.3 Numpy를 이용하여 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lv5qUyZInzoE",
    "outputId": "26cf2b25-468a-4b49-ebc8-7e353635d2c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 :\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    " # Numpy의 슬라이싱(slicing)을 사용\n",
    "np_array = np.arange(0,16).reshape((4,4))\n",
    "print('전체 데이터 :')\n",
    "print(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oQ1cdugn4jc",
    "outputId": "fa1ec9de-302f-4f96-de6b-2b68d0181530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 데이터 :\n",
      "[[ 0  1  2]\n",
      " [ 4  5  6]\n",
      " [ 8  9 10]\n",
      " [12 13 14]]\n",
      "y 데이터 : [ 3  7 11 15]\n"
     ]
    }
   ],
   "source": [
    "X = np_array[:, :3]\n",
    "y = np_array[:,3]\n",
    "\n",
    "print('X 데이터 :')\n",
    "print(X)\n",
    "print('y 데이터 :',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSKqDiKPpnjv"
   },
   "source": [
    "### 3.테스트 데이터 분리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "et0y5CP4p3Rc"
   },
   "source": [
    "####3.1 사이킷 런을 이용하여 분리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufKSifSAqVyt"
   },
   "source": [
    "학습용 테스트와 테스트용 데이터를 쉽게 분리할 수 있게 해주는 train_test_split()를 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "Yyj72SS2qW3p"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ngyfFBDqjGs"
   },
   "source": [
    "```\n",
    "X : 독립 변수 데이터. (배열이나 데이터프레임)\n",
    "y : 종속 변수 데이터. 레이블 데이터.\n",
    "test_size : 테스트용 데이터 개수를 지정한다. 1보다 작은 실수를 기재할 경우, 비율을 나타낸다.\n",
    "train_size : 학습용 데이터의 개수를 지정한다. 1보다 작은 실수를 기재할 경우, 비율을 나타낸다.\n",
    "random_state : 난수 시드\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sk997Ofxqqez",
    "outputId": "95b5f6fc-d6f1-4907-f352-0da9a351d4c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 전체 데이터 :\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n",
      "y 전체 데이터 :\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# 임의로 X와 y 데이터를 생성\n",
    "X, y = np.arange(10).reshape((5, 2)), range(5)\n",
    "\n",
    "print('X 전체 데이터 :')\n",
    "print(X)\n",
    "print('y 전체 데이터 :')\n",
    "print(list(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3h-OTO4rAmz"
   },
   "source": [
    "7:3의 비율로 데이터를 분리 <br>\n",
    "train_test_split()은 기본적으로 데이터의 순서를 섞고나서 훈련 데이터와 테스트 데이터를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "lmTqyYAlrEEB"
   },
   "outputs": [],
   "source": [
    "# 7:3의 비율로 훈련 데이터와 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZhWc3SBGrLfd",
    "outputId": "fec6da38-97eb-4c24-c7fd-df56bf478b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 훈련 데이터 :\n",
      "[[2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "X 테스트 데이터 :\n",
      "[[8 9]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 70%의 비율로 분리\n",
    "print('X 훈련 데이터 :')\n",
    "print(X_train)\n",
    "# 30%의 비율로 분리\n",
    "print('X 테스트 데이터 :')\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hv0ixVWIrTz3",
    "outputId": "0bfef728-b762-4aa6-80fb-86acfbb6fac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y 훈련 데이터 :\n",
      "[1, 2, 3]\n",
      "y 테스트 데이터 :\n",
      "[4, 0]\n"
     ]
    }
   ],
   "source": [
    "print('y 훈련 데이터 :')\n",
    "print(y_train)\n",
    "print('y 테스트 데이터 :')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4H-d6egerXFk",
    "outputId": "2e88b876-1feb-4785-fde8-09f244aa75df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y 훈련 데이터 :\n",
      "[4, 0, 3]\n",
      "y 테스트 데이터 :\n",
      "[2, 1]\n"
     ]
    }
   ],
   "source": [
    "# random_state의 값을 변경\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "print('y 훈련 데이터 :')\n",
    "print(y_train)\n",
    "print('y 테스트 데이터 :')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebVHWYA5rdc0",
    "outputId": "3983b08e-a448-4be1-c22f-cb1f737a098f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y 훈련 데이터 :\n",
      "[1, 2, 3]\n",
      "y 테스트 데이터 :\n",
      "[4, 0]\n"
     ]
    }
   ],
   "source": [
    "# random_state을 이전의 값이었던 1234로 변경\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n",
    "\n",
    "print('y 훈련 데이터 :')\n",
    "print(y_train)\n",
    "print('y 테스트 데이터 :')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36i0JHQ2rhun"
   },
   "source": [
    "#### 3.2 수동으로 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_L233V1Zrmft",
    "outputId": "b41a05a1-f8ae-4b3e-f4e1-03827982d3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 전체 데이터 :\n",
      "[[ 0  1]\n",
      " [ 2  3]\n",
      " [ 4  5]\n",
      " [ 6  7]\n",
      " [ 8  9]\n",
      " [10 11]\n",
      " [12 13]\n",
      " [14 15]\n",
      " [16 17]\n",
      " [18 19]\n",
      " [20 21]\n",
      " [22 23]]\n",
      "y 전체 데이터 :\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "# 실습을 위해 임의로 X와 y가 이미 분리 된 데이터를 생성\n",
    "X, y = np.arange(0,24).reshape((12,2)), range(12)\n",
    "\n",
    "print('X 전체 데이터 :')\n",
    "print(X)\n",
    "print('y 전체 데이터 :')\n",
    "print(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tf3X6bforoVG",
    "outputId": "fc92eff1-9828-4fd6-c415-f6203c7e41a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : 9\n",
      "테스트 데이터의 크기 : 3\n"
     ]
    }
   ],
   "source": [
    "num_of_train = int(len(X) * 0.8) # 데이터의 전체 길이의 80%에 해당하는 길이값을 구한다.\n",
    "num_of_test = int(len(X) - num_of_train) # 전체 길이에서 80%에 해당하는 길이를 뺀다.\n",
    "print('훈련 데이터의 크기 :',num_of_train)\n",
    "print('테스트 데이터의 크기 :',num_of_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1UucG5Xrsqv"
   },
   "source": [
    "어느 한 쪽을 먼저 계산하고 그 값만큼 제외하는 방식으로 계산 <br>\n",
    "데이터를 나눌 때는 num_of_train와 같이 하나의 변수만 사용하면 데이터의 누락을 방지할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "MydlXcH1ruLV"
   },
   "outputs": [],
   "source": [
    "X_test = X[num_of_train:] # 전체 데이터 중에서 20%만큼 뒤의 데이터 저장\n",
    "y_test = y[num_of_train:] # 전체 데이터 중에서 20%만큼 뒤의 데이터 저장\n",
    "X_train = X[:num_of_train] # 전체 데이터 중에서 80%만큼 앞의 데이터 저장\n",
    "y_train = y[:num_of_train] # 전체 데이터 중에서 80%만큼 앞의 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgw2zxWtr3HT",
    "outputId": "ce2e89af-2aa7-4fee-c5e7-a6d2039a7ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 테스트 데이터 :\n",
      "[[18 19]\n",
      " [20 21]\n",
      " [22 23]]\n",
      "y 테스트 데이터 :\n",
      "[9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "print('X 테스트 데이터 :')\n",
    "print(X_test)\n",
    "print('y 테스트 데이터 :')\n",
    "print(list(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyTrfua-r6kz"
   },
   "source": [
    "train_test_split()과 다른 점 : 데이터가 섞이지 않은 채 어느 지점에서 데이터를 앞과 뒤로 분리했다는 점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0cnqSNVpCUf"
   },
   "source": [
    "## 10) 한국어 전처리 패키지(Text Preprocessing Tools for Korean Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dq7SU3ssD8R"
   },
   "source": [
    "### 1.PyKoSpacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ku-1H0WsbKc"
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwVFgfDpwY6J"
   },
   "source": [
    "PyKoSpacing은 띄어쓰기가 되어있지 않은 문장을 띄어쓰기를 한 문장으로 변환해주는 패키지 <br>\n",
    "대용량 코퍼스를 학습하여 만들어진 띄어쓰기 딥 러닝 모델로 준수한 성능을 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "o0CdMTs7tP4p"
   },
   "outputs": [],
   "source": [
    "sent = '김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttMnvRfbtSJ2",
    "outputId": "a64d084e-81c1-40a7-b579-badc1044a4b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김철수는극중두인격의사나이이광수역을맡았다.철수는한국유일의태권도전승자를가리는결전의날을앞두고10년간함께훈련한사형인유연재(김광수분)를찾으러속세로내려온인물이다.\n"
     ]
    }
   ],
   "source": [
    "new_sent = sent.replace(\" \", '') # 띄어쓰기가 없는 문장 임의로 만들기\n",
    "print(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6y_509YtUFn",
    "outputId": "f938a83b-53a4-4369-8771-6393bc2f0116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n"
     ]
    }
   ],
   "source": [
    "from pykospacing import Spacing\n",
    "spacing = Spacing()\n",
    "kospacing_sent = spacing(new_sent) \n",
    "\n",
    "print(sent)\n",
    "print(kospacing_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPnbcKPMsy03"
   },
   "source": [
    "### 2.Py-Hanspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2-wNdPhs5Zg",
    "outputId": "424d6f7f-be60-4184-b0bc-976c38f726f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ssut/py-hanspell.git\n",
      "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-9t__93kn\n",
      "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-9t__93kn\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
      "Building wheels for collected packages: py-hanspell\n",
      "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-hanspell: filename=py_hanspell-1.1-py3-none-any.whl size=4868 sha256=280675a3e1392a771850df4ee42bb1923b4d814794ebede3855cdf1bca103baf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-iirep5we/wheels/ab/f5/7b/d4124bb329c905301baed80e2ae45aa14e824f62ebc3ec2cc4\n",
      "Successfully built py-hanspell\n",
      "Installing collected packages: py-hanspell\n",
      "Successfully installed py-hanspell-1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfi4nOf7wygm"
   },
   "source": [
    "Py-Hanspell은 네이버 한글 맞춤법 검사기를 바탕으로 만들어진 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fo-WW1l6wz9R",
    "outputId": "23af9125-c963-46ae-8863-80c5af54e4f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지\n"
     ]
    }
   ],
   "source": [
    "from hanspell import spell_checker\n",
    "\n",
    "sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\n",
    "spelled_sent = spell_checker.check(sent)\n",
    "\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUAQEJKzxCbG"
   },
   "source": [
    "띄어쓰기 또한 보정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BgtsDMK6xDpS",
    "outputId": "c45000da-a3ab-4ba2-8d40-bde9ace5bb77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김철수는 극 중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연제(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n"
     ]
    }
   ],
   "source": [
    "spelled_sent = spell_checker.check(new_sent)\n",
    "\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)\n",
    "print(kospacing_sent) # 앞서 사용한 kospacing 패키지에서 얻은 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSonnVw3sv3k"
   },
   "source": [
    "### 3.SOYNLP를 이용한 단어 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Y3sYfQCxeD4"
   },
   "source": [
    "- soynlp는 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저 <br>\n",
    "- 비지도 학습으로 단어 토큰화를 한다는 특징 <br>\n",
    "- soynlp 단어 토크나이저는 내부적으로 단어 점수 표로 동작\n",
    "- 응집 확률(cohesion probability)과 브랜칭 엔트로피(branching entropy)를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eKKATM4xgDS",
    "outputId": "fcf555ba-00e9-43dd-caa5-b520312e6e96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soynlp\n",
      "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
      "\u001b[K     |████████████████████████████████| 416 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.19.5)\n",
      "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.1.0)\n",
      "Installing collected packages: soynlp\n",
      "Successfully installed soynlp-0.0.493\n"
     ]
    }
   ],
   "source": [
    "pip install soynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E32S5EYhx1QZ"
   },
   "source": [
    "#### 3.1 신조어 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AvGq2QFx6Mn"
   },
   "source": [
    "기존의 형태소 분석기는 신조어나 형태소 분석기에 등록되지 않은 단어 같은 경우에는 제대로 구분하지 못하는 단점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AztdEgJx7_a",
    "outputId": "601007c6-7f5f-4a0e-9820-a84ed62ba9ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "tokenizer = Okt()\n",
    "print(tokenizer.morphs('에이비식스 이대휘 1월 최애돌 기부 요정'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OFV9SzPyFF4"
   },
   "source": [
    "텍스트 데이터에서 특정 문자 시퀀스가 함께 자주 등장하는 빈도가 높고, <br>\n",
    "앞 뒤로 조사 또는 완전히 다른 단어가 등장하는 것을 고려해서 해당 문자 시퀀스를 형태소라고 판단하는 단어 토크나이저"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y5xuDxayLCb"
   },
   "source": [
    "#### 3.2 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RajrDkR-yUNc"
   },
   "source": [
    "soynlp는 기본적으로 학습에 기반한 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "mJSdzy7XyVgx"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqCM6mCQyW6E",
    "outputId": "ed5fad8d-1f34-4d89-ad84-bd2315afcc75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2016-10-20.txt', <http.client.HTTPMessage at 0x7f9c458fc510>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습에 필요한 한국어 문서를 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"2016-10-20.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9BUkqDbybnN",
    "outputId": "149d5f8a-ce75-4ac0-cbd3-1f79c08f6449"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30091"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터를 다수의 문서로 분리\n",
    "corpus = DoublespaceLineCorpus(\"2016-10-20.txt\")\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVQK3US8ye2-",
    "outputId": "46a6ff20-5024-49d0-96f5-f018b5a6f4dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19  1990  52 1 22\n",
      "오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다 독자제공 영상 캡처 연합뉴스  서울 연합뉴스 김은경 기자 사제 총기로 경찰을 살해한 범인 성모 46 씨는 주도면밀했다  경찰에 따르면 성씨는 19일 오후 강북경찰서 인근 부동산 업소 밖에서 부동산업자 이모 67 씨가 나오기를 기다렸다 이씨와는 평소에도 말다툼을 자주 한 것으로 알려졌다  이씨가 나와 걷기 시작하자 성씨는 따라가면서 미리 준비해온 사제 총기를 이씨에게 발사했다 총알이 빗나가면서 이씨는 도망갔다 그 빗나간 총알은 지나가던 행인 71 씨의 배를 스쳤다  성씨는 강북서 인근 치킨집까지 이씨 뒤를 쫓으며 실랑이하다 쓰러뜨린 후 총기와 함께 가져온 망치로 이씨 머리를 때렸다  이 과정에서 오후 6시 20분께 강북구 번동 길 위에서 사람들이 싸우고 있다 총소리가 났다 는 등의 신고가 여러건 들어왔다  5분 후에 성씨의 전자발찌가 훼손됐다는 신고가 보호관찰소 시스템을 통해 들어왔다 성범죄자로 전자발찌를 차고 있던 성씨는 부엌칼로 직접 자신의 발찌를 끊었다  용의자 소지 사제총기 2정 서울 연합뉴스 임헌정 기자 서울 시내에서 폭행 용의자가 현장 조사를 벌이던 경찰관에게 사제총기를 발사해 경찰관이 숨졌다 19일 오후 6시28분 강북구 번동에서 둔기로 맞았다 는 폭행 피해 신고가 접수돼 현장에서 조사하던 강북경찰서 번동파출소 소속 김모 54 경위가 폭행 용의자 성모 45 씨가 쏜 사제총기에 맞고 쓰러진 뒤 병원에 옮겨졌으나 숨졌다 사진은 용의자가 소지한 사제총기  신고를 받고 번동파출소에서 김창호 54 경위 등 경찰들이 오후 6시 29분께 현장으로 출동했다 성씨는 그사이 부동산 앞에 놓아뒀던 가방을 챙겨 오패산 쪽으로 도망간 후였다  김 경위는 오패산 터널 입구 오른쪽의 급경사에서 성씨에게 접근하다가 오후 6시 33분께 풀숲에 숨은 성씨가 허공에 난사한 10여발의 총알 중 일부를 왼쪽 어깨 뒷부분에 맞고 쓰러졌다  김 경위는 구급차가 도착했을 때 이미 의식이 없었고 심폐소생술을 하며 병원으로 옮겨졌으나 총알이 폐를 훼손해 오후 7시 40분께 사망했다  김 경위는 외근용 조끼를 입고 있었으나 총알을 막기에는 역부족이었다  머리에 부상을 입은 이씨도 함께 병원으로 이송됐으나 생명에는 지장이 없는 것으로 알려졌다  성씨는 오패산 터널 밑쪽 숲에서 오후 6시 45분께 잡혔다  총격현장 수색하는 경찰들 서울 연합뉴스 이효석 기자 19일 오후 서울 강북구 오패산 터널 인근에서 경찰들이 폭행 용의자가 사제총기를 발사해 경찰관이 사망한 사건을 조사 하고 있다  총 때문에 쫓던 경관들과 민간인들이 몸을 숨겼는데 인근 신발가게 직원 이모씨가 다가가 성씨를 덮쳤고 이어 현장에 있던 다른 상인들과 경찰이 가세해 체포했다  성씨는 경찰에 붙잡힌 직후 나 자살하려고 한 거다 맞아 죽어도 괜찮다 고 말한 것으로 전해졌다  성씨 자신도 경찰이 발사한 공포탄 1발 실탄 3발 중 실탄 1발을 배에 맞았으나 방탄조끼를 입은 상태여서 부상하지는 않았다  경찰은 인근을 수색해 성씨가 만든 사제총 16정과 칼 7개를 압수했다 실제 폭발할지는 알 수 없는 요구르트병에 무언가를 채워두고 심지를 꽂은 사제 폭탄도 발견됐다  일부는 숲에서 발견됐고 일부는 성씨가 소지한 가방 안에 있었다\n",
      "테헤란 연합뉴스 강훈상 특파원 이용 승객수 기준 세계 최대 공항인 아랍에미리트 두바이국제공항은 19일 현지시간 이 공항을 이륙하는 모든 항공기의 탑승객은 삼성전자의 갤럭시노트7을 휴대하면 안 된다고 밝혔다  두바이국제공항은 여러 항공 관련 기구의 권고에 따라 안전성에 우려가 있는 스마트폰 갤럭시노트7을 휴대하고 비행기를 타면 안 된다 며 탑승 전 검색 중 발견되면 압수할 계획 이라고 발표했다  공항 측은 갤럭시노트7의 배터리가 폭발 우려가 제기된 만큼 이 제품을 갖고 공항 안으로 들어오지 말라고 이용객에 당부했다  이런 조치는 두바이국제공항 뿐 아니라 신공항인 두바이월드센터에도 적용된다  배터리 폭발문제로 회수된 갤럭시노트7 연합뉴스자료사진\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for document in corpus:\n",
    "  if len(document) > 0:\n",
    "    print(document)\n",
    "    i = i+1\n",
    "  if i == 3:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_eD6S5wylJy"
   },
   "source": [
    "기존의 KoNLPy에서 제공하는 형태소 분석기들과는 달리 학습 과정을 거쳐야 한다. <br>\n",
    "전체 코퍼스로부터 응집 확률과 브랜칭 엔트로피 단어 점수표를 만드는 과정 <br>\n",
    "WordExtractor.extract()를 통해서 전체 코퍼스에 대해 단어 점수표를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wn2Y8GhEyt2j",
    "outputId": "2f091c4a-8f00-4407-8281-45221b29bd3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 5.119 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n",
      "CPU times: user 1min 53s, sys: 938 ms, total: 1min 54s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOYFDsvGyPDq"
   },
   "source": [
    "#### 3.3 SOYNLP의 응집 확률(cohesion probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmqQ-9hey6mc"
   },
   "source": [
    "- 응집 확률은 내부 문자열(substring)이 얼마나 응집하여 자주 등장하는지를 판단하는 척도 <br>\n",
    ">문자열을 문자 단위로 분리하여 내부 문자열을 만드는 과정에서 <br>\n",
    ">왼쪽부터 순서대로 문자를 추가하면서 각 문자열이 주어졌을 때<br>\n",
    ">그 다음 문자가 나올 확률을 계산하여 누적곱을 한 값\n",
    "- 이 값이 높을수록 전체 코퍼스에서 이 문자열 시퀀스는 하나의 단어로 등장할 가능성이 높다\n",
    "\n",
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0_Ye7VizwbX",
    "outputId": "b7d4c6c1-1998-497e-c190-746f6cf5add3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08838002913645132"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table[\"반포한\"].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwyCBowV0FHR",
    "outputId": "895c2b0b-c238-4193-8b23-5d3f668ea70c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19841268168224552"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table[\"반포한강\"].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgpOCr3S0HcK",
    "outputId": "baf5da7c-3a29-49d4-baa0-442ff872a57c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2972877884078849"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table[\"반포한강공\"].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIuJS8qY0Id1",
    "outputId": "c20d75be-385e-410c-d06b-642f57c03f7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37891487632839754"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 응집확률이 제일 높다\n",
    "word_score_table[\"반포한강공원\"].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1UgdgKHU0NbE",
    "outputId": "7c469faf-18a0-4896-8c06-7e886d6b5f06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33492963377557666"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table[\"반포한강공원에\"].cohesion_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDTr2lKM0PNB"
   },
   "source": [
    "하나의 단어로 판단하기에 가장 적합한 문자열은 '반포한강공원'이라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uYlF2qPyRVH"
   },
   "source": [
    "#### 3.4 SOYNLP의 브랜칭 엔트로피(branching entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fq9KFKUa0XyK"
   },
   "source": [
    "Branching Entropy는 확률 분포의 엔트로피값을 사용 <br>\n",
    "주어진 문자열에서 얼마나 다음 문자가 등장할 수 있는지를 판단하는 척도 <br>\n",
    "하나의 완성된 단어에 가까워질수록 문맥으로 인해 점점 정확히 예측할 수 있게 되면서 점점 줄어드는 양상을 보인다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezD1b51G011D",
    "outputId": "076b10a7-4464-4e1f-cf45-07ba5ab1f3f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6371694761537934"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '디스' 다음에는 다양한 문자가 올 수 있으니까 1.63\n",
    "word_score_table[\"디스\"].right_branching_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcNzYDZX03ES",
    "outputId": "d78dd31f-74e3-439f-c72c-bc06150de2b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '디스플'이라는 문자열 다음에는 다음 문자로 '레'가 오는 것이 너무나 명백\n",
    "word_score_table[\"디스플\"].right_branching_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8q5A92PS1IHz",
    "outputId": "c33f0a9d-d243-4303-d934-418bb3033450"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table[\"디스플레\"].right_branching_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXIYtnS_1JOZ",
    "outputId": "4776251d-eb51-4349-a3df-e8dc97bf9570"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1400392861792916"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '디스플레이'라는 문자 시퀀스 다음에는 조사나 다른 단어와 같은 다양한 경우가 있을 수 있기 때문\n",
    "word_score_table[\"디스플레이\"].right_branching_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJRclw6j1Wqd"
   },
   "source": [
    "하나의 단어가 끝나면 그 경계 부분부터 다시 브랜칭 엔트로피 값이 증가<br>\n",
    "=> 이 값으로 단어를 판단하는 것이 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8SuZzaM0U8x"
   },
   "source": [
    "#### 3.5 SOYNLP의 L tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSkSpW8-1Ctn"
   },
   "source": [
    "L 토큰 + R 토큰의 형식 ('공원에' = '공원 + 에') <br>\n",
    "분리 기준을 점수가 가장 높은 L 토큰을 찾아내는 원리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQgXSd_B1jBX",
    "outputId": "a1820910-d221-42dd-9b7d-47e8bb39b84e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('국제사회', '와'), ('우리', '의'), ('노력', '들로'), ('범죄', '를'), ('척결', '하자')]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
    "l_tokenizer = LTokenizer(scores=scores)\n",
    "l_tokenizer.tokenize(\"국제사회와 우리의 노력들로 범죄를 척결하자\", flatten=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiDDnfF-09WS"
   },
   "source": [
    "#### 3.6 최대 점수 토크나이저"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cZ_A4VP1qcv"
   },
   "source": [
    "띄어쓰기가 되지 않는 문장에서 점수가 높은 글자 시퀀스를 순차적으로 찾아내는 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vx2E4SLP1wN0",
    "outputId": "9f5cfd67-38d3-4145-85b6-b57272ad5aeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 띄어쓰기가 되어 있지 않은 문장을 넣어서 점수를 통해 토큰화 된 결과\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "maxscore_tokenizer.tokenize(\"국제사회와우리의노력들로범죄를척결하자\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-VWo5_Jso2o"
   },
   "source": [
    "### 4.SOYNLP를 이용한 반복되는 문자 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LUrLWJc13Qh"
   },
   "source": [
    "ㅋㅋ, ㅋㅋㅋ, ㅋㅋㅋㅋ와 같은 경우를 모두 서로 다른 단어로 처리하는 것은 불필요 <br>\n",
    "반복되는 것은 하나로 정규화시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95lNxErc19OG",
    "outputId": "8367d2dc-42ee-468b-9433-8eb62a4173a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n"
     ]
    }
   ],
   "source": [
    "from soynlp.normalizer import *\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠ', num_repeats=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ubwI2tX2AHa",
    "outputId": "e9acaf4c-a6a0-4fda-833b-3a0ab3ad7f2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "와하하핫\n",
      "와하하핫\n",
      "와하하핫\n"
     ]
    }
   ],
   "source": [
    "print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))\n",
    "print(repeat_normalize('와하하하하하하핫', num_repeats=2))\n",
    "print(repeat_normalize('와하하하하핫', num_repeats=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypl9z7w6skht"
   },
   "source": [
    "### 5.Customized KoNLPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XxILANf2FGJ"
   },
   "source": [
    "형태소 분석기를 사용해서 단어 토큰화\n",
    "```\n",
    "형태소 분석 입력 : '은경이는 사무실로 갔습니다.'\n",
    "형태소 분석 결과 : ['은', '경이', '는', '사무실', '로', '갔습니다', '.']\n",
    "```\n",
    "형태소 분석기에 사용자 사전을 추가해줄 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUbs6UBc7LO5",
    "outputId": "71881767-9db7-4849-8c9e-65e128ad1f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting customized_konlpy\n",
      "  Downloading customized_konlpy-0.0.64-py3-none-any.whl (881 kB)\n",
      "\u001b[K     |████████████████████████████████| 881 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Jpype1>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from customized_konlpy) (1.3.0)\n",
      "Requirement already satisfied: konlpy>=0.4.4 in /usr/local/lib/python3.7/dist-packages (from customized_konlpy) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from Jpype1>=0.6.1->customized_konlpy) (3.7.4.3)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy>=0.4.4->customized_konlpy) (4.2.6)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy>=0.4.4->customized_konlpy) (1.19.5)\n",
      "Installing collected packages: customized-konlpy\n",
      "Successfully installed customized-konlpy-0.0.64\n"
     ]
    }
   ],
   "source": [
    "pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ghcb2Jc7aN5"
   },
   "source": [
    "customized_konlpy에서 제공하는 형태소 분석기 Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTFhZcR57bgf",
    "outputId": "b2f42bc5-51b1-46c2-d01c-869b250189db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/konlpy/tag/_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['은', '경이', '는', '사무실', '로', '갔습니다', '.']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "twitter.morphs('은경이는 사무실로 갔습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n_KyIUB7eCC"
   },
   "source": [
    "Twitter에 add_dictionary('단어', '품사')와 같은 형식으로 사전 추가를 해줄 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "cKVf3uSl7fxy"
   },
   "outputs": [],
   "source": [
    "twitter.add_dictionary('은경이', 'Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kN87Q2PE7igL",
    "outputId": "b038a722-8207-437e-f7ec-6c2e4e8e7127"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['은경이', '는', '사무실', '로', '갔습니다', '.']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'은경이'라는 단어가 제대로 하나의 토큰으로 인식\n",
    "twitter.morphs('은경이는 사무실로 갔습니다.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOPg95+Y8rIgh9qZ4Lj2eNS",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Text preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "imok",
   "language": "python",
   "name": "imok"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
